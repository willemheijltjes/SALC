% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%

\usepackage{graphicx}
\usepackage{opendeduction}
\usepackage{tikz}
\usetikzlibrary{decorations.pathmorphing}
\usepackage{amsmath, amssymb}
\usepackage{stmaryrd}
\usepackage{marvosym}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\let\vec\relax
\usepackage{MnSymbol}

\newcommand\defn{\textbf}

\newcommand{\FALC}{\Lambda^{S}_{a}}
\newcommand{\SLC}{\Lambda^{S}}
\newcommand{\WEAK}{\Lambda_{\weaksymbol}}
\newcommand{\fv}[1]{(#1)_{fv}}
\newcommand{\bv}[1]{(#1)_{bv}}
\newcommand{\fp}[1]{(#1)_{fp}}
\newcommand{\bp}[1]{(#1)_{bp}}
\newcommand{\fc}[1]{(#1)_{fc}}
\newcommand{\bc}[1]{(#1)_{bc}}
\newcommand{\set}[1]{ \{ #1 \} }
\newcommand{\abs}[2]{\lambda #1 . #2}
\newcommand{\app}[2]{#1 \, #2}
\newcommand{\fake}[3]{#1 \langle \, #2 \, \rangle . #3}
\newcommand{\share}[3]{#1 [#2 \leftarrow #3]}
\newcommand{\dist}[5]{#1 [ #2 \, \vert \, \fakedist{#4}{#5} \, #3 ]}
\newcommand{\olddist}[5]{#1 [ #2 \twoheadleftarrow \lambda #3 \langle #4 \rangle #5]}
\newcommand{\fakedist}[2]{#1 \langle \, #2 \, \rangle}
\newcommand{\barr}[2]{#1 \, \vert \, #2}
\newcommand{\size}[1]{\vert \, #1 \, \vert}
%\newcommand{\vecdist}[2]{\overrightarrow{\fakedist{#1}{#2} \,}}
\newcommand\vecdist[2]{\vec{#2}}
\newcommand{\sub}[3]{#1 \{ #2 / #3 \}}
\newcommand{\lamsub}[2]{\{ \lambda #1 / \lambda #2 \}}
\newcommand{\psub}[3]{#1 \{ #2 / #3 \}_{b}}
\newcommand{\exor}[3]{#1 \{ \fakedist{#2}{#3} \}_{e}}
\newcommand{\readback}[2]{\llbracket \, #1 \, \rrbracket}
\newcommand{\compile}[1]{\llparenthesis \, #1 \, \rrparenthesis}
\newcommand{\weaksymbol}{\mbox{\tiny $\mathcal{W}$}}
\newcommand{\trans}[1]{\llbracket \, #1 \, \rrbracket}
\newcommand{\tranclos}[1]{\| #1 \|}
\newcommand{\readbackclose}[1]{\llbracket \, #1 \, \rrbracket }
\newcommand{\readbackwmap}[3]{\llbracket \, #1 \, \vert \, #2 \, \vert \, #3  \, \rrbracket }
\newcommand{\readweakwmap}[3]{\llbracket \, #1 \, \vert \, #2 \, \vert \, #3  \, \rrbracket_{\weaksymbol} }
\newcommand{\bindvars}[1]{\parallel#1\parallel}
\newcommand{\compweak}[1]{\llparenthesis \, #1 \, \rrparenthesis^{\weaksymbol}}
\newcommand{\readbackweak}[1]{\lfloor \, #1 \, \rfloor}
\newcommand{\composeweak}[1]{\llbracket \, #1 \, \rrbracket^{\weaksymbol}}
\newcommand{\height}[2]{\mathcal{H}^{#1}(#2)}
\newcommand{\weight}[2]{\mathcal{W}^{#1}(#2)}
\newcommand{\weightvar}[2]{\mathcal{V}^{#1}(#2)}
\newcommand{\weightvarshare}[2]{\mathcal{F}^{#1}(#2)}

\newcommand{\fakesymbol}{\text{{$\lambda$}}}

\newcommand{\distrule}{d}
\newcommand{\switchrule}{s}
\newcommand{\sharerule}{\triangle}
\newcommand{\weakrule}{\triangle_{0}}
\newcommand{\apprule}{@}
\newcommand{\lamrule}{\lambda}

\newcommand{\byprop}[1]{\stackrel{\hbox{\tiny #1}}{\hbox{=}}}

\newcommand{\IH}{\stackrel{\hbox{\tiny I.H.}}{\hbox{=}}}

\colorlet{myGreen}{green!30!black}

\newcommand{\lambdabar}{\mbox{\textipa{\textcrlambda}}}

\def\infinity{\rotatebox{90}{8}}

\newcommand\BA{{\scriptstyle B\!\raisebox{.5ex}{$\scriptstyle A$}}}
\newcommand\imp{\mathbin\rightarrow}
\newcommand\con{\mathbin\wedge}
\newcommand\adbmal{\reflectbox{$\lambda$}}
\newcommand\red{\color{red}}
\newcommand\blue{\color{blue}}
\newcommand\black{\color{black}}
\newcommand\dirL{{\scriptscriptstyle\shortleftarrow}}
\newcommand\dirR{{\scriptscriptstyle\shortrightarrow}}
\newcommand\dirRL{{\scriptscriptstyle\leftrightarrow}}
\newcommand\dirSTOP{{\scriptscriptstyle\bot}}
\newcommand\var{{\scriptstyle\square}}

\makeatletter

\newcommand\trm[1]{%
  \vphantom(%
  \let\term@loop=\term@next%
  \let\term@end=\relax%
  \term@loop#1:%
}
\newcommand\term@next[1]{%
  \ifx#1:\let\term@loop\term@end\else%
  \ifx#1_\let\term@loop\term@sub\else%
  \ifx#1^\let\term@loop\term@sup\else%
  \ifx#1!\let\term@loop\term@vec\else%
  \ifx#1-\kern1pt{\leftarrow}\else%
  \ifx#10\nil\else%
  \ifx#1+\tight\plus\else%
  \ifx#1<\langle\else%
  \ifx#1>\rangle\else%
  #1%
  \fi\fi\fi\fi\fi\fi\fi\fi\fi%
  \term@loop%
}
\newcommand\term@sub[1]{_{#1}\let\term@loop\term@next\term@loop}
\newcommand\term@sup[1]{^{#1}\let\term@loop\term@next\term@loop}
\newcommand\term@vec[1]{\vec{\kern.5pt#1\kern.5pt}\let\term@loop\term@next\term@loop}

\newcommand*{\QEDB}{\hfill\ensuremath{\square}}

\usepackage{lipsum}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\makeatother

% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Spinal Atomic Lambda-Calculus}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Tom Gundersen\inst{1} \and
Willem Heijltjes\inst{2}\and
Michel Parigot\inst{3} \and
David Sherratt\inst{4} (\Letter)}
%
\authorrunning{Gundersen et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{
        Red Hat, Inc. \\ \email{teg@jklm.no} 
\\ \and	University of Bath, UK \\ \email{w.b.heijltjes@bath.ac.uk}
\\ \and Institut de Recherche en Informatique Fondamentale, CNRS, Universit\'e de Paris \\ \email{parigot@irif.fr}
\\ \and Friedrich-Schiller-Universit\"{a}t Jena, Germany \\ \email{david.rhys.sherratt@uni-jena.de}}
%
\maketitle              % typeset the header of the contribution
%
\vspace{-1cm}
\blfootnote{This work was supported by EPSRC Project EP/R029121/1 \emph{Typed Lambda-Calculi with Sharing and Unsharing} and ANR project 15-CE25-0014 \emph{The Fine Structure of Formal Proof Systems and their Computational Interpretations (FISP)}}

\begin{abstract}
We present the spinal atomic $\lambda$-calculus, a typed $\lambda$-calculus with explicit sharing and atomic duplication that achieves spinal full laziness: duplicating only the direct paths between a binder and bound variables is enough for beta reduction to proceed. We show this calculus is the result of a Curry--Howard style interpretation of a deep-inference proof system, and prove that it has natural properties with respect to the $\lambda$-calculus: confluence and preservation of strong normalisation.

\keywords{Lambda-Calculus \and Full laziness \and Deep inference \and Curry--Howard}
\end{abstract}

\section{Introduction}

In the $\lambda$-calculus, a main source of efficiency is \emph{sharing}: multiple use of a single subterm, commonly expressed through graph reduction~\cite{Wadsworth-1971} or explicit substitution \cite{Abadi-Cardelli-Curien-Levy-1991}. This work, and the \emph{atomic $\lambda$-calculus}~\cite{Gundersen-Heijltjes-Parigot-2013-LICS} on which it builds, is an investigation into sharing as it occurs naturally in intuitionistic \emph{deep-inference} proof theory~\cite{Tiu-2006}. The atomic $\lambda$-calculus arose as a Curry--Howard interpretation of a deep-inference proof system, in particular of the \emph{distribution} rule given below left, a variant of the characteristic \emph{medial} rule~\cite{Brunnler-Tiu-2001,Tiu-2006}. In the term calculus, the corresponding \emph{distributor} enables duplication to proceed \emph{atomically}, on individual constructors, in the style of sharing graphs \cite{Lamping-1990}. As a consequence, the natural reduction strategy in the atomic $\lambda$-calculus is \emph{fully lazy}~\cite{Wadsworth-1971,Balabonski-2012}: it duplicates only the minimal part of a term, the \emph{skeleton}, that can be obtained by lifting out subterms as explicit substitutions.
%
%\footnote{}
(While duplication is atomic \emph{locally}, a duplicated abstraction does not form a redex until also its bound variables have been duplicated; hence duplication becomes fully lazy \emph{globally}.)

	\[
		\text{\small Distribution:}
	\quad
		\drv{A\rightarrow (B\wedge C);-[d];(A\rightarrow B)\wedge(A\rightarrow C)}
	\qquad\qquad
		\text{\small Switch:}
	\quad
		\drv{(A\rightarrow B)\wedge C;-[s];A\rightarrow(B\wedge C)}
	\]

We investigate the computational interpretation of another characteristic deep-inference proof rule: the \emph{switch} rule above right~\cite{Tiu-2006}.%
\footnote{The switch rule is an intuitionistic variant of \emph{weak} or \emph{linear distributivity}~\cite{Cockett-Seely-1997} for multiplicative linear logic.}
Our result is the \emph{spinal atomic $\lambda$-calculus}, a $\lambda$-calculus with a refined form of full laziness, \emph{spine duplication}. In the terminology of \cite{Balabonski-2012}, this strategy duplicates only the \emph{spine} of an abstraction: the paths to its bound variables in the syntax tree of the term.%
\footnote{There is a clash of (existing) terminology: the \emph{spine of an abstraction}, as we use here, is a different notion from the \emph{spine of a $\lambda$-term}, which is the path from the root to the leftmost variable, as used e.g.\ in head reduction and abstract machines.} 

We illustrate these notions in Figure \ref{fig:derivationsbalunbal}, for the example $\lambda x.\lambda y.((\lambda z.z)y)x$. The \emph{scope} of the abstraction $\lambda x$ is the entire subterm, $\lambda y.((\lambda z.z)y)x$ (which may or may not be taken to include $\lambda x$ itself). Note that with explicit substitution, the scope may grow or shrink by lifting explicit substitutions in or out. The \emph{skeleton} is the term $\lambda x.\lambda y.(wy)x$ where the subterm $\lambda z.z$ is lifted out as an (explicit) substitution $[\lambda z.z/w]$. The \emph{spine} of a term, indicated in the second image, cannot naturally be expressed with explicit substitution, though one can get an impression with \emph{capturing} substitutions: it would be $\lambda x.\lambda y.wx$, with the subterm $(\lambda z.z)y$ extracted by a capturing substitution $[(\lambda z.z)y/w]$. Observe that the skeleton can be described as the \emph{iterated spine}: it is the smallest subgraph of the syntax tree closed under taking the spine of each abstraction, i.e.\ that contains the spine of every abstraction it contains.

%{\small \centering   
%	\raisebox{0.25cm}{
%	\begin{tikzpicture}[auto, scale = 0.75]
%		\filldraw[fill = orange!20, draw = orange!70, rounded corners] (-0.3, -2.75) rectangle (0.3, -4);
%		%%%%%%%%%%%%%
%		\node (a) at (0, 0) {$\lambda x$};
%		\node (b) at (0, -0.75) {$\lambda y$};
%		\node (c) at (0, -1.5) {$@$};
%		\node (d) at (0, -2.25) {$@$};
%		\node (e) at (0, -3) {$\lambda z$};
%		\node (f) at (0, -3.75) {$z$};
%		\node (g) at (0.75, -3) {$y$};
%		\node (h) at (1.5, -2.25) {$x$};
%		%%%%%%%%%%%%%
%		\draw [very thick, blue, dotted] (a) to (b);
%		\draw [very thick, blue, dotted] (b) to (c);
%		\draw [very thick, blue, dotted] (c) to (d);
%		\draw [very thick, blue, rounded corners, dotted] (d) -| (g);
%		\draw [very thick, blue, rounded corners, dotted] (c) -| (h);
%		\draw (d) to (e);
%		\draw (e) to (f);
%	\end{tikzpicture}}
%	}
%	\hspace{1cm}
%{\small   \begin{tikzpicture}[auto, scale = 0.75]
%		\filldraw[fill = yellow!20, draw = yellow!70, rounded corners] (-0.55, -2) rectangle (1.125, -4.25);
%		\filldraw[fill = orange!20, draw = orange!70, rounded corners] (-0.3, -2.75) rectangle (0.3, -4);
%		%%%%%%%%%%%%%
%		\node (a) at (0, 0) {$\lambda x$};
%		\node (b) at (0, -0.75) {$\lambda y$};
%		\node (c) at (0, -1.5) {$@$};
%		\node (d) at (0, -2.25) {$@$};
%		\node (e) at (0, -3) {$\lambda z$};
%		\node (f) at (0, -3.75) {$z$};
%		\node (g) at (0.75, -3) {$y$};
%		\node (h) at (1.5, -2.25) {$x$};
%		%%%%%%%%%%%%%
%		\draw [very thick, red, dashed] (a) to (b);
%		\draw [very thick, red, dashed] (b) to (c);
%		\draw (c) to (d);
%		\draw [rounded corners] (d) -| (g);
%		\draw [very thick, red, rounded corners, dashed] (c) -| (h);
%		\draw (d) to (e);
%		\draw (e) to (f);
%	\end{tikzpicture}}
%	\hspace{1cm}
%	\raisebox{1.5cm}{
%	\begin{tabular}{l c}
%		{\small Skeleton} &
%		 \raisebox{0.5ex}{\begin{tikzpicture}[auto]
%			\draw [blue, very thick, dotted] (0, 0) to (1, 0);
%		\end{tikzpicture}}  \\[0.2cm]
%		{\small Spine} & \raisebox{0.5ex}{{\begin{tikzpicture}[auto]
%			\draw [red, very thick, dashed] (0, 0) to (1, 0);
%		\end{tikzpicture}}} \\[0.2cm]
%		{\small Subterms} &
%		 \raisebox{-1ex}{\begin{tikzpicture}[auto]
%			\filldraw[fill = yellow!20, draw = yellow!70, rounded corners] (0, 0) rectangle (0.5, 0.5);
%			\filldraw[fill = orange!20, draw = orange!70, rounded corners] (0+0.75, 0) rectangle (0.5+0.75, 0.5);
%		\end{tikzpicture}}
%	\end{tabular}}


These notions give rise to four natural duplication regimes. For a shared abstraction to become available as the function in a $\beta$-redex: \emph{laziness} duplicates its \emph{scope} \cite{Launchbury-1993}; \emph{Full laziness} duplicates its \emph{skeleton} \cite{Wadsworth-1971}; \emph{Spinal full laziness} duplicates its \emph{spine} \cite{blanc2005sharing}; \emph{optimal reduction} duplicates only the abstraction $\lambda x$ and its bound variables $x$ \cite{Lamping-1990,Asperti-Guerrini-1998}.%
\footnote{Interestingly, Balabonski~\cite{balabonski13} shows that for \emph{weak} reduction (where one does not reduce under an abstraction) full laziness and spinal full laziness are both optimal (in the number of beta-steps required to reach a normal form).}

While each of these duplication strategies has been expressed in graphs and labelled calculi, the atomic $\lambda$-calculus is the first term calculus with Curry--Howard corresponding proof system to naturally describe full laziness. Likewise, the spinal atomic $\lambda$-calculus presented here is the first term calculus with Curry--Howard corresponding proof system to naturally describe spinal full laziness.

\paragraph*{Switch and Spine.}
%
One way to describe the skeleton or the spine of an abstraction within a $\lambda$-term is through explicit end-of-scope markers, as explored by Berkling and Fehr~\cite{Berkling-Fehr-1982}, and more recently by Hendriks and Van Oostrom \cite{Hendriks-VanOostrom-2003}. We use their \emph{adbmal} ($\adbmal$) to illustrate the idea: the constructor $\adbmal x.N$ indicates that the subterm $N$ does not contain occurrences of $x$ (or that any that do occur are not available to a binder $\lambda x$ outside $\adbmal x.N$). The scope of an abstraction thus becomes explicitly indicated in the term. This opens up a distinction between \emph{balanced} and \emph{unbalanced} scopes: whether scopes must be properly nested, or not; for example, in $\lambda x.\lambda y.N$, a subterm $\adbmal y.\adbmal x.M$ is balanced, but $\adbmal x.\adbmal y.M$ is not. With balanced scope, one can indicate the skeleton of an abstraction; with unbalanced scope (which Hendriks and Van Oostrom dismiss) one can indicate the spine. We do so for our example term $\lambda x.\lambda y.((\lambda z.z)y)x$ below. 
%
\[
\begin{array}{l@{\qquad}l}
		\text{Balanced scope/skeleton:}   & {\blue\underline{\lambda x.\lambda y.(\adbmal y.}}(\adbmal x.\lambda z.z){\blue\underline{y)(\adbmal y.x)}} 
\\[5pt]	\text{Unbalanced scope/spine:} &  {\red\underline{\lambda x.\lambda y.}}(\adbmal x.(\adbmal y.\lambda z.z)y){\red\underline{(\adbmal y.x)}}
\end{array}
\]



%\begin{array}{ll}
%  \text{Balanced:} & \text{Unbalanced:} \\
%  {\blue\underline{\lambda x. \lambda y. (\adbmal y.(}}\adbmal x.\lambda z.z{\blue\underline{)y)(\adbmal y.x)}}
%& {\red\underline{\lambda x. \lambda y. (}}\adbmal x. (\adbmal y.\lambda z.z)y{\red\underline{)(\adbmal y.x)}}
%\end{array}
%\]
%
A closely related approach is \emph{director strings}, introduced by Kennaway and Sleep \cite{Kennaway-Sleep-1988} for combinator reduction and generalized to any reduction strategy by Fern\'{a}ndez, Mackie, and Sinot in \cite{Fernandez-Mackie-Sinot-2005}. The idea is to use nameless abstractions identified by their nesting (as with De Bruijn indices), and make the paths to bound variables explicit by annotating each constructor with a string of \emph{directors}, that outline the paths. The primary aim of these approaches is to eliminate $\alpha$-conversion and to streamline substitution. Consequently, while they can \emph{identify} the spine, they do not readily isolate it for duplication. %, as would be required for spinal full laziness.
%\[
%\text{\small Director strings:}~~\red\lambda.(\lambda.({\black((\lambda.\var)\var)^\dirR}\var)^{\dirR\black\dirL})^\dirR
%\]

The present work starts from our observation that the \emph{switch} rule of open deduction functions as a proof-theoretic end-of-scope construction (see \cite{sherratt2019atomic} for details). However, it does so in a \emph{structural} way: it forces a deconstruction of a proof into readily duplicable parts, which together may form the spine of an abstraction. The derivations in Figure~\ref{fig:derivationsbalunbal} demonstrate this, as we will now explain---see the next section for how they are formally constructed.

% We illustrate this with two typing derivations for our example, in Figure \ref{fig:derivationsbalunbal}, where $x$ has type $A$ and $y,z$ type $A\imp B$, shortened to $\BA$. For a formal definition of how these derivations are constructed, see the next section.


\begin{figure}[t]
\begin{center}
\scalebox{0.75}{%
\drv{
 \drv[orange]{\top;-[\lambda];\BA\imp\BA}
 ;-[\lambda];
 A\imp
 \drv[blue!60!white]{
  (\BA\rightarrow\BA)\con A
  ;-[\lambda];
  \BA\imp
  \drv[blue!80!white]{
   (\BA\imp\BA)\con A\con\BA
   ;.;
   \drv[blue]{(\BA\imp\BA)\con\BA;-[@];\BA}\con A
   ;-[@];
   B
}}}}
\hfill
\raisebox{-1.5cm}{\scalebox{0.8}{\begin{tikzpicture}[x=20pt,y=24pt]%[auto, scale = 0.75]
		\filldraw[fill = orange!20, draw = orange!70, rounded corners] (-0.3, -2.75) rectangle (0.3, -4);
		%%%%%%%%%%%%%
		\node (a) at (0, 0) {$\lambda x$};
		\node (b) at (0, -0.75) {$\lambda y$};
		\node (c) at (0, -1.5) {$@$};
		\node (d) at (0, -2.25) {$@$};
		\node (e) at (0, -3) {$\lambda z$};
		\node (f) at (0, -3.75) {$z$};
		\node (g) at (0.75, -3) {$y$};
		\node (h) at (1.5, -2.25) {$x$};
		%%%%%%%%%%%%%
		\draw [very thick, blue, dotted] (a) to (b);
		\draw [very thick, blue, dotted] (b) to (c);
		\draw [very thick, blue, dotted] (c) to (d);
		\draw [very thick, blue, rounded corners, dotted] (d) -| (g);
		\draw [very thick, blue, rounded corners, dotted] (c) -| (h);
		\draw (d) to (e);
		\draw (e) to (f);
	\end{tikzpicture}}}
\hfill	
\scalebox{0.75}{%
\drv{
 \drv[red]{\top;-[a];A\imp A}
 \con
 \drv[yellow!60!white]{
  \top
  ;-[a];
  \BA\imp\drv[yellow]{
   \BA
   ;.;
   \drv[orange]{\top;-[a];\BA\imp\BA}
   \con\BA
   ;-[@];\BA
 }}
 ;-[s];
 \drv[red!60!white]{
  A\imp
  \drv[red!80!white]{
   A\con(\BA\imp\BA)
   ;.;
   (\BA\imp\BA)\con A
   ;-[s];
   \BA\imp\drv[red]{\BA\con A;-[@];B}
}}}}
\hfill
\raisebox{-1.7cm}{\scalebox{0.8}{\begin{tikzpicture}[x=20pt,y=24pt]%[auto, scale = 0.75]
		\filldraw[fill = yellow!20, draw = yellow!70, rounded corners] (-0.55, -2) rectangle (1.125, -4.25);
		\filldraw[fill = orange!20, draw = orange!70, rounded corners] (-0.3, -2.75) rectangle (0.3, -4);
		%%%%%%%%%%%%%
		\node (a) at (0, 0) {$\lambda x$};
		\node (b) at (0, -0.75) {$\lambda y$};
		\node (c) at (0, -1.5) {$@$};
		\node (d) at (0, -2.25) {$@$};
		\node (e) at (0, -3) {$\lambda z$};
		\node (f) at (0, -3.75) {$z$};
		\node (g) at (0.75, -3) {$y$};
		\node (h) at (1.5, -2.25) {$x$};
		%%%%%%%%%%%%%
		\draw [very thick, red, dashed] (a) to (b);
		\draw [very thick, red, dashed] (b) to (c);
		\draw (c) to (d);
		\draw [rounded corners] (d) -| (g);
		\draw [very thick, red, rounded corners, dashed] (c) -| (h);
		\draw (d) to (e);
		\draw (e) to (f);
	\end{tikzpicture}}}
\end{center}
\caption{Balanced and unbalanced typing derivations for $\lambda x.\lambda y.((\lambda z.z)y)x$, with corresponding graphical representations of the term. The variable $x$ has type $A$ and $y,z$ type $A\imp B$, shortened to $\BA$. The left derivation isolates the skeleton of $\lambda x$, in blue, and the right derivation its spine, in red.}
\label{fig:derivationsbalunbal}
\end{figure}

The abstraction $\lambda x$ corresponds in the proof system to the implication $A\imp$, explicitly scoping over its right-hand side. On the left, with the \emph{abstraction} rule $(\lambda)$, scopes must be balanced, and the proof system may identify the \emph{skeleton}; here, that of $\lambda x$ as the largest blue box. Decomposing the abstraction $(\lambda)$ into \emph{axiom} $(a)$ and \emph{switch} $(s)$, on the right the proof system may express unbalanced scope. It does so by separating the scope of an abstraction into multiple parts; here, that of $\lambda x$ is captured as the two top-level red boxes. Each box is ready to be duplicated; in this way, one may duplicate the spine of an abstraction only.

These two derivations correspond to terms in our calculus. The subterms not part of the skeleton (i.e.\ $\lambda z . z$) remain shared and we are able to duplicate the skeleton alone. This is also possible in \cite{Gundersen-Heijltjes-Parigot-2013-LICS}. In our calculus we are also able to duplicate just the spine by using a \emph{distributor}. We require this construct as otherwise we break the binding of the $y$-abstraction. The distributor manages and maintains these bindings. The $y$-abstraction in the spine ($y \langle a \rangle$) is a \emph{phantom-abstraction}, because it is not real and we cannot perform $\beta$-reduction on it. However, it may become real during reduction. It can be seen as a placeholder for the abstraction. The variables in the \emph{cover} ($a$) represent subterms that both remain shared and are found in the distributor.

\[
\begin{array}{l@{\qquad}l}
		\text{Skeleton:} & {\blue\underline{\lambda x. \lambda y. (a\,y)\,x}}\, [a \leftarrow \lambda z . z]
\\[5pt]	\text{Spine:}	 &  {\red\underline{\lambda x. y\langle a\rangle. (a)\,x}}\, [y \langle a \rangle \, \vert\, \lambda y . \,  [a \leftarrow (\lambda z . z)y ]]
\end{array}
\]


Our investigation is then focused on the interaction of switch and distribution (later observed in the rewrite rule \ref{red:distshare}). The use of the distribution rule allows us to perform duplication atomically, and thus provides a natural strategy for spinal full laziness. In Figure \ref{fig:derivationsbalunbal} on the right, this means duplicating the two top-level red boxes can be done independently from duplicating the yellow box.

%In Section \ref{sec:typingacalculus}, we introduce a simple sharing calculus that we expand on in Section \ref{chap:salc}, where we introduce the syntax and semantics of the spinal atomic $\lambda$-calculus, and its typing system. In Section \ref{chap:snosr} we further study the reduction rules that allow for spinal duplication, and prove natural properties of these rules such as termination and confluence. In Section \ref{chap:posn} we extend these results to include beta reduction, and show preservation of strong normalisation with respect to the $\lambda$-calculus. We conclude in Section \ref{chap:conc}.

\section{Typing a $\lambda$-calculus in open deduction}

\label{sec:typingacalculus}

We work in \emph{open deduction}~\cite{Guglielmi-Gundersen-Parigot-2010}, a formalism of deep-inference proof theory, using the following proof system for (conjunction--implication) intuitionistic logic. A \emph{derivation} from a \emph{premise} formula $X$ to a \emph{conclusion} formula $Z$ is constructed inductively as in Figure \ref{fig:derivations}, with from left to right: a propositional atom $a$, where $X = Z = a$; \emph{horizontal composition} with a connective $\rightarrow$, where $X = Y \rightarrow X_{2}$ and $Z = Y \rightarrow Z_{2}$; \emph{horizontal composition} with a connective $\wedge$, where $X = X_{1} \wedge X_{2}$ and $Z = Z_{1} \wedge Z_{2}$; and \emph{rule composition}, where $r$ is an inference rule (Figure \ref{fig:abstraction}) from $Y_{1}$ to $Y_{2}$. The boxes serve as parentheses (since derivations extend in two dimensions) and may be omitted. Derivations are considered up to associativity of rule composition. One may consider formulas as derivations that omit rule composition. We work modulo associativity, symmetry, and unitality of conjunction, justifying the $n$-ary contraction, and may omit $\top$ from the axiom rule. A $0$-ary contraction, with conclusion $\top$, is a \emph{weakening}. Figure \ref{fig:abstraction}: the abstraction rule ($\lambda$) is derived from axiom and switch. \emph{Vertical composition} of a derivation from $X$ to $Y$ and one from $Y$ to $Z$, depicted by a dashed line, is a defined operation, given in Figure \ref{fig:vertical}, where $* \in \set{\wedge, \rightarrow}$.

\begin{figure}[t]
\centering
  \begin{subfigure}[b]{0.425\textwidth}
    \scalebox{0.85}{$
      \drv{X;|;Z}
      ~{:}{:}{=}~ a
      ~\mid~	  Y \imp \drv[yellow]{X_{2};|;Z_{2}}
      ~\mid~	  \drv[yellow]{X_1\!;|;Z_1\!} \con 
      			  \drv[yellow]{X_2\!;|;Z_2\!}
      ~\mid~ \drv{\drv[yellow]{X;|;Y_1\!} ;-[r];
      			  \drv[yellow]{Y_2\!;|;Z}}
    $}
    \caption{Derivations}
    \label{fig:derivations}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.55\textwidth}
	\scalebox{0.85}{$
	  \begin{array}{@{}c@{\qquad}c@{\qquad}c@{}}
		  \drv{\top\vphantom(;-[a];X\imp X}
		& \drv{(X\imp Y)\con X;-[@];Y}
		& \drv{X\vphantom(;-[\sharerule];X\con\dots\con X}
		\\ \\
		  \drv{(X\imp Y)\con Z;-[s];X\imp(Y\con Z)}
		& \multicolumn{2}{@{}c@{}}{
			\drv{X\vphantom(;-[\lambda];Y\imp(X\con Y)} ~~{:}{=}~~
			\drv{
			  \drv[yellow]{\top;-[a];Y\imp Y} \con X 
			  ;-[s]; 
			  Y\imp(X\con Y)
		  }}
	  \end{array}
	$}
	\caption{Inference rules: axiom ($a$), application ($@$), contraction ($\sharerule$), switch ($s$), abstraction ($\lambda$)}
	\label{fig:abstraction}
	\end{subfigure}
	\\[0.2cm]
	\begin{subfigure}[b]{\textwidth}
\scalebox{0.75}{ 
\drv{\drv[yellow]{X ; | ; Y} ; . ; \drv[yellow]{Y ; | ; Z}} 
	~{:}{=}}
\hfill
\scalebox{0.75}{
\drv{X ; . ; \drv[yellow]{X ; | ; Z}} $=$ \drv{\drv[yellow]{X ; | ; Z} ; . ; Z} $=$ \drv{X ; | ; Z}}
\hfill
 \scalebox{0.75}{\drv{\drv[yellow]{\drv[magenta]{X_{1} ; | ; X_{2}} ; -[r] ; \drv[green]{Y ; | ; Z_{1}}} ; . ; \drv[cyan]{Z_{1} ; | ; Z_{2}}} $=$ \drv{ \drv[magenta]{X_{1} ; | ; X_{2}} ; -[r] ; \drv[yellow]{ \drv[green]{Y ; | ; Z_{1}} ; . ; \drv[cyan]{Z_{1} ; | ; Z_{2}}}}
} 
\hfill
 \scalebox{0.75}{\drv{ \drv[magenta]{X_{1} ; | ; X_{2}} ; . ; \drv[yellow]{ \drv[green]{X_{2} ; | ; Y} ; -[r] ; \drv[cyan]{Z_{1} ; | ; Z_{2}}}} $=$ \drv{\drv[yellow]{\drv[magenta]{X_{1} ; | ; X_{2}} ; . ; \drv[green]{X_{2} ; | ; Y}} ; -[r] ; \drv[cyan]{Z_{1} ; | ; Z_{2}}}
} 
 \hfill
\scalebox{0.75}{
\drv{ \drv[yellow]{\drv[magenta]{X_{1} ; | ; Y_{1}} * \drv[orange]{X_{2} ; | ; Y_{2}}} ; . ; \drv[yellow]{\drv[cyan]{Y_{1} ; | ; Z_{1}} * \drv[green]{Y_{2} ; | ; Z_{2}}}}
=
\drv{\drv[yellow]{ \drv[magenta]{X_{1} ; | ; Y_{1}} ; . ; \drv[cyan]{Y_{1} ; | ; Z_{1}}} * \drv[yellow]{ \drv[orange]{X_{2} ; | ; Y_{2}} ; . ; \drv[green]{Y_{2} ; | ; Z_{2}} }}
}
	\caption{Vertical composition}
	\label{fig:vertical}
	\end{subfigure}
%	\begin{subfigure}[b]{0.2\textwidth}
%	\centering
%		{\small \drv{Y \rightarrow \drv[yellow]{X ; | ; Z}}}
%	\caption{Implication}
%	\label{fig:implication}
%	\end{subfigure}
%	\hfill
	\caption{Intuitionistic proof system in open deduction}
\end{figure}

\subsection{The Sharing Calculus}

Our starting point is the \emph{sharing calculus} ($\SLC$), a calculus with an explicit sharing construct, similar to explicit substitution.

\begin{definition}
\label{def:sharingcalsyntax}
The \defn{pre-terms} $r,s,t,u$ and \defn{sharings} $[\Gamma]$ of the $\SLC$ are defined by:
\[
	s,t 
	~{:}{:}{=}~ x 
	~\mid~ \abs xt 
	~\mid~ \app st 
	~\mid~ t[\Gamma] 
\qquad
	[\Gamma] ~{:}{:}{=}~ \share{}{x_{1}, \dots, x_{n}}{s}
\]
with from left to right: a \defn{variable}; an \defn{abstraction}, where $x$ occurs free in $t$ and becomes bound; an \defn{application}, where $s$ and $t$ use distinct variable names; and a \defn{closure}; in $\share{t}{\vec{x}}{s}$ the variables in the vector $\vec{x} = x_{1}, \dots, x_{n}$ all occur in $t$ and become bound, and $s$ and $t$ use distinct variable names. \defn{Terms} are pre-terms modulo \defn{permutation} equivalence ($\sim$):
\[
	\share{t}{\vec{x}}{s} \share{}{\vec{y}}{r} \sim t \share{}{\vec{y}}{r} \share{}{\vec{x}}{s} \quad \quad (\set{\vec{y}} \cap \fv{s} = \set{} )
\]
A term is in \defn{sharing normal form} if all sharings occur as $\share{}{\vec{x}}{x}$ either at the top level or directly under a binding abstraction, as $\abs{x}{\share{t}{\vec{x}}{x}}$.
\end{definition}

\noindent Note that variables are \emph{linear}: variables occur at most once, and bound variables must occur. A vector $\vec{x}$ has length $\size{\vec{x}}$ and consist of the variables $x_{1}, \dots, x_{\size{\vec{x}}}$. An \defn{environment} is a sequence of sharings $\overline{[\Gamma]} = [\Gamma_{1}] \dots [\Gamma_{n}]$. Substitution is written $\sub{}{t}{x}$, and $\sub{}{t_{1}}{x_{1}} \dots \sub{}{t_{n}}{x_{n}}$ may be abbreviated to $\sub{}{t_{i}}{x_{i}}_{i \in [n]}$.

\begin{definition}
The \defn{interpretation} $\readbackclose{-} : \Lambda \rightarrow \SLC$ is defined below.
\[\readbackclose{x} = x \quad \readbackclose{\abs{x}{t}} = \abs{x}{\readbackclose{t}} \quad \readbackclose{\app{s}{t}} = \app{\readbackclose{s}}{\readbackclose{t}} \quad \readbackclose{\share{t}{\vec{x}}{s}} = \readbackclose{t} \sub{}{\readbackclose{s}}{x_{i}}_{i \in [n]}\]
\end{definition}

The \defn{translation} $\compile{N}$ of a $\lambda$-term $N$ is the unique sharing-normal term $t$ such that $N = \readbackclose{t}$. A term $t$ will be typed by a derivation with restricted types, as shown below, where the \emph{context type} $\Gamma = A_{1} \wedge \dots \wedge A_{n}$ will have an $A_{i}$ for each free variable $x_{i}$ of $t$. We connect free variables to their premises by writing $A^{x}$ and $\Gamma^{\vec{x}}$. The $\SLC$ is then typed as in Figure \ref{fig:SLCT}.

\begin{figure}[t]
\begin{center}
		Basic Types:  \quad $A, B, C ~ {:}{=} ~ a \mid A \rightarrow B$ 
\hfill	Context Types:\quad $\Gamma, \Delta, \Omega ~{:}{=}~ A \mid \top \mid \Gamma \wedge \Delta$
\end{center}
$x :$ \scalebox{0.9}{ \drv{A^{x}}}
\hfill
$\app{t}{s} :$ \scalebox{0.9}{ \drv{\drv[yellow]{\Gamma ; |[t] ; A \rightarrow B} \wedge \drv[yellow]{\Delta ; |[s] ; A} ; -[\apprule] ; B}}
\hfill
$\abs{x}{t} :$ \scalebox{0.9}{ \drv{\Gamma ; -[\lamrule] ; A \rightarrow \drv[yellow]{\Gamma \wedge A^{x} ; |[t] ; B}}}
\hfill
$\share{t}{\vec{x}}{s} :$ \scalebox{0.9}{ \drv{\Gamma \wedge \drv[yellow]{\Delta ; |[s] ; A ; -[\sharerule] ; A \wedge \dots \wedge A} ; . ; \Gamma \wedge (A \wedge \dots \wedge A)^{\vec{x}} ; |[t] ; B}}
\caption{Typing system for $\SLC$}
\label{fig:SLCT}
\end{figure}

\section{The Spinal Atomic $\lambda$-Calculus}
\label{chap:salc}

We now formally introduce the syntax of the spinal atomic $\lambda$-calculus ($\FALC$), by extending the definition of the sharing calculus in Definition \ref{def:sharingcalsyntax} with a \emph{distributor} construct that allows for atomic duplication of terms.

\begin{definition}[Pre-Terms] The \defn{pre-terms} $r, s, t$, \defn{closures} $[\Gamma]$, and \defn{environments} $\overline{[\Gamma]}$ of the $\FALC$ are defined by:

\begin{center}
$t \quad {:}{:}{=} \quad x \quad \vert \quad \app{s}{t} \quad \vert \quad \fake{x}{\vec{y}}{t} \quad \vert \quad t[\Gamma]$ \quad \quad \quad $\overline{[\Gamma]} \quad {:}{:}{=} \quad [\Gamma] \quad \vert \quad \overline{[\Gamma]}[\Gamma]$
%\end{center}
%\begin{center}
\\[0.1cm]
%$[\Gamma] \quad {:}{:}{=} \quad  [\vec{x} \leftarrow t] \quad \vert \quad \dist{}{\fakedist{e_{1}}{\vec{x_{1}}} \dots \fakedist{e_{n}}{\vec{x_{n}}}}{\overline{[\Gamma]}}{d}{\vec{y}}$ 
$[\Gamma] \quad {:}{:}{=} \quad  [\vec{x} \leftarrow t] \quad \vert \quad \dist{}{\vec x}{\overline{[\Gamma]}}{y}{\vec z}$ 
\end{center}

\end{definition}

Our generalized abstraction $\fake x{\vec y}t$ is a \defn{phantom-abstraction}, where $x$ a \defn{phantom-variable} and the \defn{cover} $\vec y$ will be a subset of the free variables of $t$. It can be thought of as a ``delayed'' abstraction: $x$ is a binder, but possibly not in $t$ itself, and instead in the terms substituted for the variables $\vec y$; in other words, $x$ is a \emph{capturing} binder for substitution into $\vec y$. We define standard $\lambda$-abstraction as the special case $\abs{x}{t} \equiv \fake{x}{x}{t}$, and generally, when we refer to $\fakedist x{\vec y}$ as a phantom-abstraction (rather than an abstraction) we assume $\vec y\neq x$. The \defn{distributor} $\dist{u}{\vec x}{\overline{[\Gamma]}}{y}{\vec z}$ binds the phantom-variables $\vec x$ in $u$, while its environment $\overline{[\Gamma]}$ will bind the variables in their covers; intuitively, it represents a set of explicit substitutions in which the variables $\vec x$ are expected to be captured.

\begin{figure}[t]
\centering
	\begin{subfigure}[b]{0.6\textwidth}
		\begin{tikzpicture}[auto, scale = 0.75]
		%%%%%%%%%%%%%
		\node (a) at (0, 0.75) {$\circ$};
		\node (b) at (0, 0) {$\lambda x$};
		\node (c) at (0, -0.75) {$\lambda y$};
		%%%%%%%%%%%%%
		\draw (-0.5, 1.25) to (a); \draw (0.5, 1.25) to (a);
		\draw (a) to (b); \draw (b) to (c); \draw (c) to (0, -1.25);
		\draw [rounded corners] (0.75, -1.25) |- (c);
		\draw [rounded corners] (1.5, -1.25) |- (b);
\end{tikzpicture}
\hspace{0.2cm}
\begin{tikzpicture}[auto, scale = 0.75]
		%%%%%%%%%%%%%
		\node (b1) at (-0.5, 0.75) {\color{blue} $\lambda x_{1}$};
		\node (b2) at (0.5, 0.75) {\color{blue} $\lambda x_{1}$};
		\node (a) at (0, 0) {$\circ$};
		\node (b3) at (1.5, 0) {$\bullet$};
		\node (c) at (0, -0.75) {$\lambda y$};
		%%%%%%%%%%%%%
		\draw (-0.5, 1.25) to (b1); \draw (0.5, 1.25) to (b2);
		\draw (b1) to (a); \draw (b2) to (a);
		\draw (a) to (c); \draw (c) to (0, -1.25);
		\draw [rounded corners] (0.75, -1.25) |- (c);
		\draw (1.5, -1.25) to (b3); 
		\draw [dotted, thick, red] (b3) to (a);
		\draw [dashed, blue, thick, rounded corners] (b3) |- (b2);
		\draw [dashed, blue, thick] (b2) -- (b1);
\end{tikzpicture}
\hspace{0.2cm}
\begin{tikzpicture}[auto, scale = 0.75]
		%%%%%%%%%%%%%
		\node (b1) at (-0.5, 0.75) {\color{blue} $\lambda x_{1}$};
		\node (b2) at (0.5, 0.75) {\color{blue} $\lambda x_{2}$};
		\node (c1) at (-0.5, 0) {\color{myGreen} $\lambda y_{1}$};
		\node (c2) at (0.5, 0) {\color{myGreen} $\lambda y_{2}$};
		\node (b3) at (2.25, 0) {$\bullet$};
		\node (c3) at (1.5, -0.75) {$\bullet$};
		\node (a) at (0, -0.75) {$\circ$};
		%%%%%%%%%%%%%
		\draw (-0.5, 1.25) to (b1); \draw (0.5, 1.25) to (b2);
		\draw (b1) to (c1); \draw (b2) to (c2);
		\draw (c1) to (a); \draw (c2) to (a);
		\draw (a) to (0, -1.25);
		\draw [dotted, red, thick] (b3) to (a); \draw [dotted, red, thick] (c3) to (a);
		\draw (1.5, -1.25) to (c3);
		\draw [dashed, myGreen, thick, rounded corners] (c3) |- (c2);
		\draw [dashed, myGreen, thick] (c2) to (c1);
		\draw (2.25, -1.25) to (b3); 
		\draw [dashed, blue, thick, rounded corners] (b3) |- (b2);
		\draw [dashed, blue, thick] (b2) -- (b1);
\end{tikzpicture}
	\caption{Distributor introduction}
	\label{fig:distintro}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.35\textwidth}
	\begin{tikzpicture}[auto, scale = 0.75]
		%%%%%%%%%%%%%
		\node (a) at (0, 0.75) {$\circ$};
		\node (c) at (0, 0) {$@$};
		\node (b) at (1.5, 0.75) {$\bullet$};
		%%%%%%%%%%%%%
		\draw (-0.5, 1.5) to (a); \draw (0.5, 1.5) to (a);
		\draw (a) to (c);
		\draw (c) to (0, -0.5);
		\draw [rounded corners] (c) -| (0.75, -0.5);
		\draw [red, dotted, thick] (b) to (a);
		\draw (1.5, -0.5) to (b);
		\draw [blue, dashed, thick] (b) to (1.5, 1.5);
\end{tikzpicture}
\hspace{0.25cm}
\begin{tikzpicture}[auto, scale = 0.75]
		%%%%%%%%%%%%%
		\node (c1) at (-0.5, 0.75) {$@$}; \node (c2) at (0.5, 0.75) {$@$};
		\node (a1) at (-0.5, 0) {$\circ$}; \node (a2) at (0.5, 0) {$\circ$};
		\node (b) at (1.5, 0.375) {$\bullet$};
		%%%%%%%%%%%%%
		\draw (-0.5, 1.5) to (b1); \draw (0.5, 1.5) to (b2);
		\draw [blue, dashed, thick] (b) to (1.5, 1.5);
		\draw (a1) to (c1); \draw (a1) to (c2);  \draw (a2) to (c1);  \draw (a2) to (c2); 
		\draw (a1) to (-0.5, -0.5); \draw (a2) to (0.5, -0.5); \draw (1.5, -0.5) to (b);
		\draw [red, dotted, thick, bend left = 45] (b) to (a1); \draw [red, dotted, thick] (b) to (a2);
\end{tikzpicture}
	\caption{Application duplication}
	\label{fig:duplcapp}
	\end{subfigure}
	\\[0.1cm]
	\begin{subfigure}[b]{0.35\textwidth}
	\begin{tikzpicture}[auto, scale = 0.75]
		%%%%%%%%%%%%%
		\node (a) at (0, 0) {$\circ$};
		\node (b) at (1, 0) {$\bullet$};
		\node (c) at (0, -0.75) {$t$};
		%%%%%%%%%%%%%
		\draw (-0.5, 0.75) to (a); \draw (0.5, 0.75) to (a);
		\draw (a) to (c);
		\draw (c) to (0, -1.25);
		\draw (b) to (1, -1.25);
		\draw [red, thick, dotted] (b) to (a);
		\draw [blue, thick, dashed] (b) to (1, 0.75);
\end{tikzpicture}
\hspace{0.2cm}
\begin{tikzpicture}[auto, scale = 0.75]
		%%%%%%%%%%%%%
		\node (a) at (0, 0) {$\circ$};
		\node (b) at (1, 0) {$\bullet$};
		\node (c) at (0, -0.75) {$t$};
		%%%%%%%%%%%%%
		\draw (-0.5, 0.75) to (a); \draw (0.5, 0.75) to (a);
		\draw (a) to (c);
		\draw (c) to (0, -1.25);
		\draw (b) to (1, -1.25); 
		\draw [blue, thick, dashed] (b) to (1, 0.75);
\end{tikzpicture}
	\caption{Lifting out of a distributor}
	\label{fig:lift}
	\end{subfigure}
	\hspace{0.3cm}
	\begin{subfigure}[b]{0.45 \textwidth}
	\begin{tikzpicture}[auto, scale = 0.75]
		%%%%%%%%%%%%%
		\node (c1) at (-0.5, 0) {\color{blue} $\lambda x_{1}$};
		\node (c2) at (0.5, 0) {\color{blue} $\lambda x_{2}$};
		\node (d1) at (-0.5, -0.75) {$t_{1}$};
		\node (d2) at (0.5, -0.75) {$t_{2}$};
		\node (a) at (0, -1.5) {$\circ$};
		\node (b) at (1.5, -0.75) {$\bullet$};
		%%%%%%%%%%%%% 
		\draw (-0.5, 0.5) to (c1); \draw (0.5, 0.5) to (c2);
		\draw (c1) to (d1); \draw (c2) to (d2);
		\draw (d1) to (a); \draw (d2) to (a);
		\draw [rounded corners] (a) |- (0.75, -2) -| (b);
		\draw [blue, dashed, thick, rounded corners] (b) |- (c2); 
		\draw [blue, dashed, thick] (c2) to (c1); 
		\draw [red, dotted, thick, bend left = 30 ] (b) to (a);
\end{tikzpicture}
\hspace{0.3cm}
\begin{tikzpicture}[auto, scale = 0.75]
		%%%%%%%%%%%%%
		\node (c1) at (-0.5, 0.25) {$\lambda x_{1}$};
		\node (c2) at (1.25, 0.25) { $\lambda x_{2}$};
		\node (d1) at (-0.5, -0.75) {$t_{1}$};
		\node (d2) at (1.25, -0.75) {$t_{2}$};
		%%%%%%%%%%%%% 
		\draw (c1) to (d1); \draw (c2) to (d2);
		\draw [rounded corners] (d1) |- (0, -1.5) -| (0.5, -0.5) |- (c1);
		\draw [rounded corners] (d2) |- (0 + 1.75, -1.5) -| (0.5 + 1.75, -0.5) |- (c2);
		\draw (-0.5, 1) to (c1); \draw (1.25, 1) to (c2);
\end{tikzpicture}
	\caption{Eliminating the distributor}
	\label{fig:distelim}
	\end{subfigure}
	\\[0.1cm]
	\begin{subfigure}{\textwidth}
\begin{tikzpicture}[auto, scale = 0.7]
		%%%%%%%%%%%%%
		\node (a) at (0, 0.75) {$\circ$};
		\node (b) at (0, 0) {$\lambda x$};
		\node (c) at (0, -0.75) {$\lambda y$};
		\node (d) at (0, -1.5) {$@$};
		%%%%%%%%%%%%%
		\draw (-0.5, 1.25) to (a); \draw (0.5, 1.25) to (a);
		\draw (a) to (b); \draw (b) to (c); \draw (c) to (0, -1.25);
		\draw [rounded corners] (0.75, -2.25) |- (c);
		\draw [rounded corners] (d) -| (1.2, -1) |- (b);
		\draw (c) to (d);
		\draw (d) to (0, -2.25);
\end{tikzpicture}
\raisebox{1cm}{$\rightarrow$}
\begin{tikzpicture}[auto, scale = 0.7]
		%%%%%%%%%%%%%
		\node (b1) at (-0.5, 0.75) {\color{blue} $\lambda x_{1}$};
		\node (b2) at (0.5, 0.75) {\color{blue} $\lambda x_{2}$};
		\node (c1) at (-0.5, 0) {\color{myGreen} $\lambda y_{1}$};
		\node (c2) at (0.5, 0) {\color{myGreen} $\lambda y_{2}$};
		\node (b3) at (2, 0) {$\bullet$};
		\node (c3) at (1.5, -0.75) {$\bullet$};
		\node (a) at (0, -0.75) {$\circ$};
		\node (d) at (0, -1.5) {$@$};
		%%%%%%%%%%%%%
		\draw (-0.5, 1.25) to (b1); \draw (0.5, 1.25) to (b2);
		\draw (b1) to (c1); \draw (b2) to (c2);
		\draw (c1) to (a); \draw (c2) to (a);
		\draw (a) to (d);
		\draw [dotted, red, thick] (b3) to (a); \draw [dotted, red, thick] (c3) to (a);
		\draw (1.5, -2.25) to (c3);
		\draw [dashed, myGreen, thick, rounded corners] (c3) |- (c2);
		\draw [dashed, myGreen, thick] (c2) to (c1);
		\draw [rounded corners] (d) -| (b3); 
		\draw [dashed, blue, thick, rounded corners] (b3) |- (b2);
		\draw [dashed, blue, thick] (b2) -- (b1);
		\draw (d) to (0, -2.25);
\end{tikzpicture}
\raisebox{1cm}{$\rightarrow$}
\begin{tikzpicture}[auto, scale = 0.7]
		%%%%%%%%%%%%%
		\node (b1) at (-0.5, 0.75) {\color{blue} $\lambda x_{1}$};
		\node (b2) at (0.5, 0.75) {\color{blue} $\lambda x_{2}$};
		\node (c1) at (-0.5, 0) {\color{myGreen} $\lambda y_{1}$};
		\node (c2) at (0.5, 0) {\color{myGreen} $\lambda y_{2}$};
		\node (b3) at (2, 0) {$\bullet$};
		\node (c3) at (1.5, -0.75) {$\bullet$};
		\node (a1) at (-0.5, -1.5) {$\circ$};
		\node (a2) at (0.5, -1.5) {$\circ$};
		\node (d1) at (-0.5, -0.75) {$@$};
		\node (d2) at (0.5, -0.75) {$@$};
		%%%%%%%%%%%%%
		\draw (-0.5, 1.25) to (b1); \draw (0.5, 1.25) to (b2);
		\draw (b1) to (c1); \draw (b2) to (c2);
		%\draw (c1) to (a); \draw (c2) to (a);
		\draw [dotted, red, thick] (b3) to (a1); \draw [dotted, red, thick] (c3) to (a1);
		\draw [dotted, red, thick, bend left = 40] (b3) to (a2); \draw [dotted, red, thick] (c3) to (a2);
		%%%%%%%%%%%%%%%%%%%%%%
		\draw (1.5, -2.25) to (c3);
		\draw [dashed, myGreen, thick, rounded corners] (c3) |- (c2);
		\draw [dashed, myGreen, thick] (c2) to (c1);
		\draw [dashed, blue, thick, rounded corners] (b3) |- (b2);
		\draw [dashed, blue, thick] (b2) -- (b1);
		\draw (c1) to (d1); \draw (c2) to (d2);
		\draw (d1) to (a1); \draw (d1) to (a2); \draw (d2) to (a1); \draw (d2) to (a2);
		\draw (a1) to (-0.5, -2.25);
		\draw [rounded corners] (a2) |- (1.25, -1.95) -| (b3);
\end{tikzpicture}
\raisebox{1cm}{$\rightarrow$}
\begin{tikzpicture}[auto, scale = 0.7]
		%%%%%%%%%%%%%
		\node (b1) at (-0.5, 0.75) {\color{blue} $\lambda x_{1}$};
		\node (b2) at (0.5, 0.75) {\color{blue} $\lambda x_{2}$};
		\node (c1) at (-0.5, 0) {\color{myGreen} $\lambda y_{1}$};
		\node (c2) at (0.5, 0) {\color{myGreen} $\lambda y_{2}$};
		\node (b3) at (2, 0) {$\bullet$};
		\node (c3) at (1.5, -0.75) {$\bullet$};
		\node (a1) at (-0.5, -1.5) {$\circ$};
		\node (a2) at (0.5, -1.5) {$\circ$};
		\node (d1) at (-0.5, -0.75) {$@$};
		\node (d2) at (0.5, -0.75) {$@$};
		%%%%%%%%%%%%%
		\draw (-0.5, 1.25) to (b1); \draw (0.5, 1.25) to (b2);
		\draw (b1) to (c1); \draw (b2) to (c2);
		%\draw (c1) to (a); \draw (c2) to (a);
		\draw [dotted, red, thick] (c3) to (a1);
		\draw [dotted, red, thick, bend left = 40] (b3) to (a2); %\draw [dotted, red, thick, bend left = 30] (c3) to (a2);
		%%%%%%%%%%%%%%%%%%%%%%
		\draw (1.5, -2.25) to (c3);
		\draw [dashed, myGreen, thick, rounded corners] (c3) |- (c2);
		\draw [dashed, myGreen, thick] (c2) to (c1);
		\draw [dashed, blue, thick, rounded corners] (b3) |- (b2);
		\draw [dashed, blue, thick] (b2) -- (b1);
		\draw (c1) to (d1); \draw (c2) to (d2);
		\draw (d1) to (a1); \draw (d1) to (a2); \draw (d2) to (a1); \draw (d2) to (a2);
		\draw (a1) to (-0.5, -2.25);
		\draw [rounded corners] (a2) |- (1.25, -1.95) -| (b3);
\end{tikzpicture}
\raisebox{1cm}{$\rightarrow$}
\begin{tikzpicture}[auto, scale = 0.7]
		%%%%%%%%%%%%%
		\node (b1) at (-0.5, 0.75) {$\lambda x_{1}$};
		\node (b2) at (0.75, 0.75) {$\lambda x_{2}$};
		\node (c1) at (-0.5, 0) {\color{myGreen} $\lambda y_{1}$};
		\node (c2) at (0.75, 0) {\color{myGreen} $\lambda y_{2}$};
		\node (c3) at (1.75, -1.5) {$\bullet$};
		\node (a1) at (-0.5, -1.5) {$\circ$};
		\node (d1) at (-0.5, -0.75) {$@$};
		\node (d2) at (0.75, -0.75) {$@$};
		%%%%%%%%%%%%%
		\draw (-0.5, 1.25) to (b1); \draw (0.5, 1.25) to (b2);
		\draw (a1) to (-0.5, -2.25);
		\draw (b1) to (c1); \draw (b2) to (c2); \draw (c1) to (d1); \draw (c2) to (d2);
		\draw [rounded corners] (d1) -| (0.25, 0) |- (b1);
		\draw [rounded corners] (d2) -| (0.25 + 1.25, 0) |- (b2);
		\draw (d1) to (a1); \draw (d2) to (a1);
		\draw (1.75, -2.25) to (c3);
		\draw [dashed, myGreen, thick] (c2) to (c1);
		\draw [dashed, myGreen, thick, rounded corners] (c3) |- (c2);
		\draw [dotted, red, thick] (a1) to (c3);
\end{tikzpicture}
\caption{Duplicating the spine}
\label{fig:reduction}
\end{subfigure}

\caption{Graphical illustration of the distributor}
\end{figure}

The distributor is introduced when we wish to duplicate an abstraction, as depicted in Figure \ref{fig:distintro}. The sharing node ($\circ$) duplicates the abstraction node, creating a distributor (depiced as the sharing and unsharing node ($\bullet$), together with the bindings of the phantom-variables (depicted with a blue dashed line). The variables captured by the environment are the variables connected to sharing nodes linked with a red dotted line. Notice one sharing node can be linked with multiple unsharing nodes, and vice versa. Duplication of applications also duplicates the red dotted line (Figure \ref{fig:duplcapp}), but these can be removed later if the term does not contain the variable bound to the unsharing (Figure \ref{fig:lift}). These subterms are those which are not part of the spine. Eventually, we will reach a state where the only sharing node connected to the unsharing node is the one that shared the variable bound to the unsharing, allowing us to eliminate the distributor (Figure \ref{fig:distelim}). The purpose of the dotted line is similar to the brackets of optimal reduction graphs \cite{Lamping-1990,van2004lambdascope}, to supervise which sharing and unsharing match.

 Terms are then pre-terms with sensible and correct bindings. To define terms, we first define \emph{free} and \emph{bound} variables and phantom variables; variables are bound by abstractions (not phantoms) and by sharings, while phantom-variables are bound by distributors. 

\begin{definition}[Free and Bound Variables]
\label{def:freeboundvar} The \defn{free variables} $\fv{-}$ and \defn{bound variables} $\bv{-}$ of a pre-term $t$ are defined as follows
\begingroup
\allowdisplaybreaks
	\begin{align*}
		\fv{x} &= \set{x} & \bv{x} &= \set{} \\
		\fv{\app{s}{t}} &= \fv{s} \cup \fv{t} & \bv{\app{s}{t}} &= \bv{s} \cup \bv{t} \\
		\fv{\fake{x}{x}{t}} &= \fv{t} - \set{x} & \bv{\fake{x}{x}{t}} &= \bv{t} \cup \set{x} \\
		\fv{\fake{x}{\vec{y}}{t}} &= \fv{t} & \bv{\fake{x}{\vec{y}}{t}} &= \bv{t} \\
		\fv{\share{u}{\vec{x}}{t}} &= \fv{u} \cup \fv{t} - \set{\vec{x}} & \bv{\share{u}{\vec{x}}{t}} &= \bv{u} \cup \bv{t} \cup \set{\vec{x}} \\
		%
		\fv{\dist{u}{\vecdist{e}{x}}{\overline{[\Gamma]}}{y}{y}} &= \fv{u \overline{[\Gamma]}} - \set{y} 
	  & \bv{\dist{u}{\vecdist{e}{x}}{\overline{[\Gamma]}}{y}{y}} &= \bv{u \overline{[\Gamma]}} \cup{\set y}\\
	  	\fv{\dist{u}{\vecdist{e}{x}}{\overline{[\Gamma]}}{y}{\vec z}} &= \fv{u \overline{[\Gamma]}} \cup \set{y} 
	  & \bv{\dist{u}{\vecdist{e}{x}}{\overline{[\Gamma]}}{y}{\vec z}} &= \bv{u \overline{[\Gamma]}}
	\end{align*}
%	\vspace{-0.8cm}
%	\begin{align*}
%		\fv{\dist{u}{\vecdist{e}{x}}{\overline{[\Gamma]}}{c}{c}} &= \fv{u \overline{[\Gamma]}} - \set{c} & \bv{\dist{u}{\vecdist{e}{x}}{\overline{[\Gamma]}}{c}{c}} &= \bv{u \overline{[\Gamma]}} \\
%		\fv{\dist{u}{\vecdist{e}{x}}{\overline{[\Gamma]}}{c}{\vec{y}}} &= \fv{u \overline{[\Gamma]}} \cup \set{c} & \bv{\dist{u}{\vecdist{e}{x}}{\overline{[\Gamma]}}{c}{\vec{y}}} &= \bv{u \overline{[\Gamma]}} 
%	\end{align*}
\endgroup
\end{definition}
\begin{definition}[Free and Bound Phantom-Variables]
\label{def:freeboundphan}
The \defn{free phantom-variables} $\fp{-}$ and \defn{bound phantom-variables} $\bp{-}$ of the pre-term $t$ are defined as follows
\begingroup
\allowdisplaybreaks
	\begin{align*}
		\fp{x} &= \set{} & \bp{x} &= \set{} \\
		\fp{\app{s}{t}} &= \fp{s} \cup \fp{t} & \bp{\app{s}{t}} &= \bp{s} \cup \bp{t} \\
		\fp{\fake{x}{x}{t}} &= \fp{t} \\ %& \bp{\fake{x}{x}{t}} &= \bp{t}  \\
		\fp{\fake{c}{\vec{x}}{t}} &= \fp{t} \cup \set{c}& \bp{\fake{c}{\vec{x}}{t}} &= \bp{t} \\
		\fp{\share{u}{\vec{x}}{t}} &= \fp{u} \cup \fp{t} & \bp{\share{u}{\vec{x}}{t}} &= \bp{u} \cup \bp{t}\\
		\fp{\dist{u}{\vec x}{\overline{[\Gamma]}}cc} &= \fp{u \overline{[\Gamma]}} - \set{\vec x} \\
	%&	\bp{\dist{u}{\vec x}{\overline{[\Gamma]}}cc} &= \bp{u \overline{[\Gamma]}} \cup \set{\vec x} \\
		\fp{\dist{u}{\vec x}{\overline{[\Gamma]}}c{\vec{y}}} &= \fp{u \overline{[\Gamma]}} \cup \set c - \set{\vec x}
	&	\bp{\dist{u}{\vec x}{\overline{[\Gamma]}}c{\vec{y}}} &= \bp{u \overline{[\Gamma]}} \cup \set{\vec x}
	\end{align*}
%	\vspace{-0.9cm}
%	\begin{align*}
%	\fp{\dist{u}{\fakedist{e_{1}}{\vec{x_{1}}} \dots \fakedist{e_{n}}{\vec{x_{n}}}}{\overline{[\Gamma]}}{c}{c}} &= \fp{u \overline{[\Gamma]}} - \set{e_{1}, \dots, e_{n}} \\
%	\bp{\dist{u}{\fakedist{e_{1}}{\vec{x_{1}}} \dots \fakedist{e_{n}}{\vec{x_{n}}}}{\overline{[\Gamma]}}{c}{c}} &= \bp{u \overline{[\Gamma]}} \cup \set{e_{1}, \dots, e_{n}} \\
%	\fp{\dist{u}{\fakedist{e_{1}}{\vec{x_{1}}} \dots \fakedist{e_{n}}{\vec{x_{n}}}}{\overline{[\Gamma]}}{c}{\vec{y}}} &= \fp{u \overline{[\Gamma]}} \cup \set{c} - \set{e_{1}, \dots, e_{n}} \\
%	\bp{\dist{u}{\fakedist{e_{1}}{\vec{x_{1}}} \dots \fakedist{e_{n}}{\vec{x_{n}}}}{\overline{[\Gamma]}}{c}{\vec{y}}} &= \bp{u \overline{[\Gamma]}} \cup \set{e_{1}, \dots, e_{n}}
%	\end{align*}
\endgroup
\end{definition}

\noindent
The \defn{free covers} $\fc{u}$ and \defn{bound covers} $\bc{u}$ are the covers associated with the free phantom-variables $\fp u$ respectively the bound phantom-variables $\bp u$ of $u$; that is, if $x$ occurs as $\fakedist x{\vec a}$ in $u$ and $x\in\fp{u}$ then $\langle\vec a\rangle\in\fc u$. When bound, $x$ and the variables in $\vec a$ may be alpha-converted independently. When a distributor $\dist u{\vec x}{\overline{[\Gamma]}}{y}{\vec{z}}$ binds the phantom-variables $\vec x=x_1,\dots,x_n$ where each $x_i$ occurs as $\fakedist{x_i}{\vec a_i}$ in $u$, then for technical convenience we may make the covers explicit in the distributor itself, and write
\[
	\dist{u}{\fakedist{x_1}{\vec a_1}\dots\fakedist{x_n}{\vec a_n}} {\overline{[\Gamma]}}{y}{\vec{z}}~.
\]
The environment $\overline{[\Gamma]}$ is expected to bind \emph{exactly} the variables in the covers $\langle\vec a_i\rangle$. We apply this and other restrictions to define the terms of the calculus.


%\noindent 
%We use the notation $\fakedist{c}{\vec{x}} \in \fc{t}$ to say that $\vec{x}$ is a free cover, and $\fakedist{c}{\vec{x}} \in \bc{t}$ to say that the cover is bound by the environment of the distributor that binds $c$. With these, we can define terms.

\begin{definition}
\label{def:falcterms}
\defn{Terms} $t \in \FALC$ are pre-terms with the following constraints
\begin{enumerate}
	\item  Each variable may occur at most once.
%	\item In an abstraction $\fake{x}{x}{t}$, $x \in \fv{t}$.
%	\item In a phantom-abstraction $\fake{c}{x_{1}, \dots, x_{n}}{t}$, $\set{x_{1}, \dots, x_{n}} \subseteq \fv{t}$.
	\item In a phantom-abstraction $\fake x{\vec y}t$, $\{\vec y\}\subseteq\fv t$.
%	\item In a sharing $\share{u}{x_{1}, \dots, x_{n}}{t}$, $\set{x_{1}, \dots, x_{n}} \subseteq \fv{u}$.
	\item In a sharing $\share{u}{\vec x}{t}$, $\set{\vec x} \subseteq \fv{u}$.
%	\item  In a distributor $\dist{u}{\fakedist{e_{1}}{w^{1}_{1}, \dots, w^{1}_{k_{1}}} \dots \fakedist{e_{n}}{w^{n}_{1}, \dots, w^{n}_{k_{n}}}}{\overline{[\Gamma]}}{c}{c}$  
%	\begin{enumerate}
%		\item For all $1 \leq i \leq n$ and $1 \leq m \leq k_{n}$, $w^{i}_{m} \fv{u}$ and are bound by $\overline{[\Gamma]}$.
%		\item $\set{\fakedist{e_{1}}{w^{1}_{1}, \dots, w^{1}_{k_{1}}}, \dots, \fakedist{e_{n}}{w^{n}_{1}, \dots, w^{n}_{k_{n}}}} \subseteq \fc{u}$.
%		\item The (not phantom-)variable $c$ occurs freely in the environment $\overline{[\Gamma]}$.
%	\end{enumerate}
%	\item  In a distributor $\dist{u}{\fakedist{e_{1}}{w^{1}_{1}, \dots, w^{1}_{k_{1}}} \dots \fakedist{e_{n}}{w^{n}_{1}, \dots, w^{n}_{k_{n}}}}{\overline{[\Gamma]}}{c}{y_{1}, \dots, y_{m}}$  
%	\begin{enumerate}
%		\item Both $5(a)$ and $5(b)$ hold.
%		\item For all $1 \leq i \leq m$, $y_{i}$ occurs in the environments $\overline{[\Gamma]}$.
%	\end{enumerate}
	\item  In a distributor $\dist{u}{\fakedist{x_1}{\vec a_1} \dots \fakedist{x_n}{\vec a_n}}{\overline{[\Gamma]}}{y}{\vec z}$  
	\begin{enumerate}
		\item $\set{x_1,\dots,x_n}\subseteq \fp u$;
		\item the variables in $\bigcup_{i\leq n}\{\vec a_i\}$ are free in $u$ and bound by $\overline{[\Gamma]}$.
		\item the variables in $\set{\vec z}$ occur freely in the environment $\overline{[\Gamma]}$.
	\end{enumerate}
\end{enumerate}

\end{definition}
\begin{example}
Here we show some pre-terms that are not terms.
\begin{itemize}
	\item $\fake{c}{x}{y}$ \quad (violates condition $2$)
	\item $\share{\app{x}{y}}{x, z}{w}$\quad (violates condition $3$)
	\item $\app{\fake{e_{2}}{w_{2}}{w_{2}}}{(\app{(\fake{e_{1}}{w_{1}}{w_{1}})}{z})} \dist{}{\fakedist{e_{1}}{w_{1}}, \fakedist{e_{2}}{w_{2}}}{\share{}{w_{1}, w_{2}}{\fake{x}{x}{\app{x}{y}}}}{c}{z}$

	(violates condition $4a$)
\end{itemize}
\end{example}

We also work modulo permutation with respect to the variables in the cover of phantom-abstractions. Let $\vec{x}$ be a list of variables and let $\vec{x_{P}}$ be a permutation of that list, then the following terms are considered equal.

\begin{center}
	$\share{u}{\vec{x}}{t} \sim \share{u}{\vec{x_{P}}}{t}$
	\hspace{1cm}
	$\fake{y}{\vec{x}}{t} \sim \fake{y}{\vec{x_{P}}}{t}$
\end{center}

\noindent Terms are typed with the typing system for $\SLC$ extended with the \emph{distribution} inference rule. This rule is the result of computationally interpreting the medial rule as done in \cite{Gundersen-Heijltjes-Parigot-2013-LICS}. We obtain this variant of the medial rule due to the restriction for implications and to avoid introducing disjunction to the typing system. The terms of $\FALC$ are then typed as in both Figure \ref{fig:SLCT} and Figure \ref{fig:derivphandist}. Note environments are typed by the derivations of all its closures composed horizontally with the conjunction connective. Also note that in the case for phantom-abstraction is similar for that of an abstraction, where we replace one occurrence of the simple type $A$ by the conjunction $\Gamma$.


\begin{figure}[t]
$\fake{c}{\vec{x}}{t}:$ \scalebox{0.8}{ \drv{(A \rightarrow \Gamma) \wedge \Delta ; -[\switchrule] ; A^{c} \rightarrow \drv[yellow]{\Gamma^{\vec{x}} \wedge \Delta ; |[t] ; C}}}
\hspace{0.8cm}
$\dist{u}{\vecdist{e}{x}}{\overline{[\Gamma]}}{c}{\vec{z}}: $ \scalebox{0.8}{ \drv{\drv{\drv[yellow]{(C \rightarrow \Gamma) \wedge \Delta ; -[\switchrule] ; C^{c} \rightarrow
	\drv[cyan]{\Gamma^{\color{black} \vec{z}} \wedge \Delta ; |[{\color{black} \overline{[\Gamma]}} ] ; \Sigma_{1} \wedge \dots \wedge \Sigma_{n}} ; -[\distrule] ; (C^{\color{black}e_{1}} \rightarrow \Sigma_{1}^{\vec{\color{black}x_{1}}}) \wedge \dots \wedge  (C^{\color{black}e_{n}} \rightarrow \Sigma_{n}^{\vec{\color{black}x_{n}}})}
	\wedge \Omega} ; . ; (C \rightarrow \Sigma_{1}) \wedge \dots \wedge (C \rightarrow \Sigma_{n}) \wedge \Omega ; |[u] ; E} }
\caption{Typing derivations for phantom-abstractions and distributors}
\label{fig:derivphandist}
\end{figure}

\subsection{Compilation and Readback.}
\label{sect:compandreadback}

We now define the translations between $\FALC$ and the original $\lambda$-calculus. First we define the interpretation $\Lambda \rightarrow \FALC$ (\emph{compilation}). Intuitively, it replaces each abstraction $\lambda x . -$ with the term $\fake{x}{x}{\share{-}{x_{1}, \dots, x_{n}}{x}}$ where $x_{1}, \dots, x_{n}$ replace the occurrences of $x$. Actual substitutions are denoted as $\sub{}{t}{x}$. Let $\size{M}_{x}$ denote the number of occurrences of $x$ in $M$, and if $\size{M}_{x} = n$ let $M \frac{n}{x}$ denote $M$ with the occurrences of $x$ replaced by fresh, distinct variables $x_{1}, \dots, x_{n}$. First, the translation of a \emph{closed} term $M$ is $\compile{M}'$, defined below

\begin{definition}[Compilation]
\label{def:compile}
The interpretation of $\lambda$ terms, $\compile{\Lambda}' : \Lambda \rightarrow \FALC$, is defined as

\[ \compile{M \frac{n_{1}}{x_{1}} \dots \frac{n_{k}}{x_{k}} }' \share{}{x^{1}_{1}, \dots, x^{n_{1}}_{1}}{x_{1}} \dots \share{}{x^{1}_{k}, \dots, x^{n_{k}}_{k}}{x_{k}} \]

where $x_{1}, \dots, x_{k}$ are the free variables of $M$ such that $\size{M}_{x_{i}} = n_{i} > 1$ and $\compile{-}'$ is defined on terms as (where $n \neq 1$ in the abstraction case):
\[
\begin{aligned}
	\compile{x}' &= x \\
	\compile{\app{M}{N}}' &= \app{\compile{M}'}{\compile{N}'}
\end{aligned}
\, \, \, \, \,
	\compile{\abs{x}{M}}' =
	\begin{cases}
		\fake{x}{x}{\compile{M}'} & \text{if } \size{M}_{x} = 1 \\
		\fake{x}{x}{\share{\compile{M \frac{n}{x}}'}{x_{1}, \dots, x_{n}}{x}} & \text{if } \size{M}_{x} = n
	\end{cases}
\]

%The interpretation for closed lambda terms, $\compile{\Lambda}' : \Lambda \rightarrow \FALC$ is defined below
%%\begin{align*}
%%	\compile{x}' &= x \\
%%	\compile{\app{M}{N}}' &= \app{\compile{M}'}{\compile{N}'} \\
%%	\compile{\abs{x}{M}}' &=
%%	\begin{cases}
%%		\fake{x}{x}{\compile{M}'} & \text{if } \size{M}_{x} = 1 \\
%%		\fake{x}{x}{\share{\compile{M \frac{n}{x}}'}{x^{1}, \dots, x^{n}}{x}} & \text{if } \size{M}_{x} = n \neq 1
%%	\end{cases}
%%\end{align*}
%\[
%\begin{aligned}
%	\compile{x}' &= x \\
%	\compile{\app{M}{N}}' &= \app{\compile{M}'}{\compile{N}'}
%\end{aligned}
%\, \, \, \,
%	\compile{\abs{x}{M}}' =
%	\begin{cases}
%		\fake{x}{x}{\compile{M}'} & \text{if } \size{M}_{x} = 1 \\
%		\fake{x}{x}{\share{\compile{M \frac{n}{x}}'}{x^{1}, \dots, x^{n}}{x}} & \text{if } \size{M}_{x} = n
%	\end{cases}
%\]
%For an arbitrary term $M$, if $x_{1}, \dots, x_{k}$ are the free variables of $M$ such that $\size{M}_{x_{i}} = n_{i} > 1$, the translation $\compile{M}$ is:
%\[
%\compile{M \frac{n_{1}}{x_{1}} \dots \frac{n_{k}}{x_{k}} }' \share{}{x^{1}_{1}, \dots, x^{n_{1}}_{1}}{x_{1}} \dots \share{}{x^{1}_{k}, \dots, x^{n_{k}}_{k}}{x_{k}}~.
%\]
\end{definition}

The readback into the $\lambda$-calculus is slightly more complicated, specifically due to the bindings induced by the distributor. Interpreting a distributor construct as a $\lambda$-term requires (1) converting the phantom-abstractions it binds in $u$ into abstractions (2) collapsing the environment (3) maintaining the bindings between the converted abstractions and the intended variables located in the environment.


\begin{definition}
Given a total function $\sigma$ with domain $D$ and codomain $C$, we \defn{overwrite} the function with case $x \mapsto v$ where $x \in D$ and $v \in C$ such that

%\[ \sigma [ x \mapsto v ] (z) = \begin{cases} v & z = x \\ \sigma(z) & \text{otherwise}  \end{cases} \]

\[\sigma [x \mapsto v] (z) \quad := \quad \text{if } (x = z) \text{ then } v \text{ else } \sigma(z)  \]

\end{definition}

We use the map $\sigma$ as part of the translation, the intuition is that for all bound variables $x$ in the term we are translating, it should be that $\sigma(x) = x$. The purpose of the map $\gamma$ is to keep track of the binding of phantom-variables.

\begin{definition}
\label{def:readback}
The interpretation $\readbackwmap{-}{-}{-} : \FALC \times (V \rightarrow \Lambda) \times (V \rightarrow V) \rightarrow \Lambda$ is defined as
\begingroup
\allowdisplaybreaks
	\[ \readbackwmap{x}{\sigma}{\gamma} = \sigma(x) \hspace{0.6cm}
	\readbackwmap{\app{s}{t}}{\sigma}{\gamma} = \app{\readbackwmap{s}{\sigma}{\gamma}}{\readbackwmap{t}{\sigma}{\gamma}} \]
	\[ \readbackwmap{\fake{c}{c}{t}}{\sigma}{\gamma} = \abs{c}{\readbackwmap{t}{\sigma[c \mapsto c]}{\gamma}} \]
	\[ \readbackwmap{\fake{c}{x_{1}, \dots, x_{n}}{t}}{\sigma}{\gamma} = \abs{c}{\readbackwmap{t}{\sigma[x_{i} \mapsto \sigma(x_{i}) \sub{}{c}{\gamma(c)}]_{i \in [n]}}{\gamma}} \]
	\[ \readbackwmap{\share{u}{x_{1}, \dots, x_{n}}{t}}{\sigma}{\gamma} = \readbackwmap{u}{\sigma[x_{i} \mapsto \readbackwmap{t}{\sigma}{\gamma}]_{i \in [n]}}{\gamma}  \]
	\[ \readbackwmap{\dist{u}{\fakedist{e_{1}}{\vec{w_{1}}}, \dots, \fakedist{e_{n}}{\vec{w_{n}}}}{\overline{[\Gamma]}}{c}{c}}{\sigma}{\gamma} = \readbackwmap{u \overline{[\Gamma]}}{\sigma}{\gamma[e_{i} \mapsto c]_{i \in [n]}} \]
	\[ \readbackwmap{\dist{u}{\fakedist{e_{1}}{\vec{w_{1}}}, \dots, \fakedist{e_{n}}{\vec{w_{n}}}}{\overline{[\Gamma]}}{c}{x_{1}, \dots, x_{m}}}{\sigma}{\gamma} = \readbackwmap{u \overline{[\Gamma]}}{\sigma'}{\gamma[e_{i} \mapsto c]_{i \in [n]}} \]
	  $\text{ where } \sigma' = \sigma[x_{i} \mapsto \sigma(x_{i}) \sub{}{c}{\gamma(c)}]_{i \in [n]}$
\endgroup

\end{definition}

\noindent The following Proposition justifies working modulo permutation equivalence.
\begin{proposition}
\label{lem:preservedenotationsim}
For $s, t \in \FALC$, if $s \sim t$ then $\trans{s} = \trans{t}$.
\end{proposition}

\subsection{Rewrite Rules.}

Both the spinal atomic $\lambda$-calculus and the atomic $\lambda$-calculus of \cite{Gundersen-Heijltjes-Parigot-2013-LICS} follow atomic reduction steps, i.e.\ they apply on individual constructors. The biggest difference is that our calculus is capable of duplicating not only the skeleton but also the spine. The rewrite rules in our calculus make use of 3 operations, \emph{substitution}, \emph{book-keeping}, and \emph{exorcism}. The operation \defn{substitution} $\sub{t}{s}{x}$ propagates through the term $t$, and replaces the free occurences of the variable $x$ with the term $s$. Moreover, if $x$ occurs in the cover of a phantom-variable $\fakedist{e}{\vec{y} \cdot x}$, then substitution replaces the $x$ in the cover with $\fv{s}$, resulting in $\fakedist{e}{\vec{y} \cdot \fv{s}}$. Although substitution performs some book-keeping on phantom-abstractions, we define an explicit notion of \defn{book-keeping} $\psub{}{\vec{y}}{e}$ that updates the variables stored in a free cover i.e.\ for a term $t$, $\fakedist{e}{\vec{x}} \in \fc{t}$ then $\fakedist{e}{\vec{y}} \in \fc{\psub{t}{\vec{y}}{e}}$. The last operation we introduce is called \defn{exorcism} $\exor{}{c}{\vec{x}}$. We perform exorcisms on phantom-abstractions to convert them to abstractions. Intuitively, this will be performed on phantom-abstractions with phantom-variables bound to a distributor when said distributor is eliminated. It converts phantom-abstractions to abstractions by introducing a sharing of the phantom-variable that captures the variables in the cover, i.e.\ $(\fake{c}{\vec{x}}{t}) \exor{}{c}{\vec{x}} = \fake{c}{c}{\share{t}{\vec{x}}{c}}$. 

\begin{proposition}
\label{prop:suboutcomm}
\label{prop:bkcomm}
\label{prop:exorcomm}
The translation $\readbackwmap{u}{\sigma}{\gamma}$ commutes with substitutions, book-keepings$^{1}$, and exorcisms$^{2}$ in the following way
	\[ \readbackwmap{u \sub{}{t}{x}}{\sigma}{\gamma} = \readbackwmap{u}{\sigma[x \mapsto \readbackwmap{t}{\sigma}{\gamma}]}{\gamma} \]
	\[ \readbackwmap{u \psub{}{\vec{x}}{c}}{\sigma}{\gamma} = \readbackwmap{u}{\sigma}{\gamma} \]
	\[ \readbackwmap{u \exor{}{c}{x_{1}, \dots, x_{n}}}{\sigma}{\gamma} = \readbackwmap{u}{\sigma[x_{i} \mapsto c]_{i \in [n]}}{\gamma} \]

\noindent {\small (1) Given $\fakedist{c}{\vec{y}} \in \fc{u}$ where $\vec{x} \subseteq \vec{y}$ and for $z \in \vec{y} / \vec{x}$, $\gamma(c) \not\in \fv{\sigma(z)}$} \\
{\small (2) Given $\fakedist{c}{\vec{x}} \in \fc{u}$ or $\{ \vec{x} \} \cap \fv{u} = \{ \} $ }

\begin{proof}
See~\cite{sherratt2019atomic}, proof of Proposition 18, 19, 20, 21.
\end{proof}

%$$\readbackwmap{u}{\sigma'}{\gamma} = \readbackwmap{u}{\sigma}{\gamma} \sub{}{M}{x}$$
%
%where $\sigma'(z) = \begin{cases} \sigma(z) \sub{}{M}{x} & z \neq x \\ M & \text{otherwise} \end{cases}$
\end{proposition}

Using these operations, we define the rewrite rules that allow for spinal duplication. Firstly we have beta reduction ($\rightsquigarrow_{\beta}$), which strictly requires an abstraction (not a phantom).
\begin{equation}  \label{red:beta} \tag{$\beta$}
\app{(\fake{x}{x}{t})}{s} \rightsquigarrow_{\beta} \sub{t}{s}{x} \hspace{1cm} 
\begin{tabular}{c c c}
	\scalebox{0.9}{ 
		\drv{\drv{\Gamma ; -[\lamrule] ; A \rightarrow \drv[yellow]{A^{x} \wedge \Gamma ; |[t] ; B}} \wedge \drv[green]{\Delta ; |[s] ; A} ; -[\apprule] ; B}
	}
	&
	$\rightsquigarrow_{\beta}$
	&
	\scalebox{0.9}{\drv{\drv{\drv[green]{\Delta ; |[{s}] ; A} \wedge \Gamma} ; . ; \drv[yellow]{A \wedge \Gamma ; |[t] ; B}}}
\end{tabular}
\end{equation}
Here $\beta$-reduction is a linear operation, since the bound variable $x$ occurs exactly once in the body $t$. Any duplication of the term $t$ in the atomic $\lambda$-calculus proceeds via the sharing reductions. 

The first set of sharing reduction rules move closures towards the outside of a term. Most of these rewrite rules only change the typing derivations in the way that subderivations are composed, with the exception of moving a closure out of scope of a distributor.
\begin{align}
\app{s[\Gamma]}{t} &\rightsquigarrow_{L} (\app{s}{t})[\Gamma]   \label{red:liftappleft} \tag{$l_{1}$} \\
\app{s}{t[\Gamma]} &\rightsquigarrow_{L} (\app{s}{t})[\Gamma] \label{red:liftappright} \tag{$l_{2}$} \\
\fake{d}{\vec{x}}{t[\Gamma]} &\rightsquigarrow_{L} (\fake{d}{\vec{x}}{t})[\Gamma]  \text{ if } \set{\vec{x}} \cap \fv{t} = \set{\vec{x}}  \label{red:liftabs} \tag{$l_{3}$} \\
\share{u}{\vec{x}}{t[\Gamma]} &\rightsquigarrow_{L} \share{u}{\vec{x}}{t}[\Gamma] \label{red:liftshare} \tag{$l_{4}$}
\end{align}
For the case of lifting a closure outside a distributor, we use a notation $\bindvars{[\Gamma]}$ to identify the variables captured by a closure, i.e.$\bindvars{\share{}{\vec{x}}{t}} = \set{\vec{x}}$ and \\ $\bindvars{\dist{}{\fakedist{e_{1}}{\vec{x_{1}}}, \dots, \fakedist{e_{n}}{\vec{x_{x}}}}{\overline{[\Gamma]}}{c}{c}} = \set{\vec{x_{1}}, \dots, \vec{x_{n}}}$. Then let $\set{\vec{z}} = \bindvars{[\Gamma]}$ in the following rewrite rule, where we remove $\vec{z}$ from the covers, that can only occur if $\set{\vec{x}} \cap \fv{[\Gamma]} = \set{}$.
\begin{equation} \label{red:distshare} \tag{$l_{5}$}
\begin{multlined}
\dist{u}{\fakedist{e_{1}}{\vec{w_{1}}} \dots \fakedist{e_{n}}{\vec{w_{n}}}}{\overline{[\Gamma]} [\Gamma]}{c}{\vec{x}} \\ \rightsquigarrow_{L} \dist{u {\psub{}{(\vec{w_{i}} \setminus \vec{z})}{e_{i}}}_{i \in [n]}}{\fakedist{e_{1}}{\vec{w_{1}} \setminus \vec{z}} \dots \fakedist{e_{n}}{\vec{w_{n}} \setminus \vec{z}}}{\overline{[\Gamma]}}{c}{\vec{x}}[\Gamma]
\end{multlined}
\end{equation}

The graphical version of this rule is shown in Figure \ref{fig:lift}, where we remove the edge only if there is no edge between $t$ and the unsharing node. The proof rewrite rule corresponding with the rewrite rule \ref{red:distshare} can be broken down into two parts. The first part is readjusting how the derivations compose as shown below.

\begin{center}
\begin{tabular}{c c c}
	\scalebox{0.8}{\drv{(C \rightarrow \Gamma) \wedge \Delta \wedge \Omega ; -[\switchrule] ; C^{\color{red} c} \rightarrow \drv[yellow]{\drv[green]{\Gamma^{\color{red} \vec{x}} \wedge \Delta \wedge \drv[cyan]{\Omega ; |[\color{red} {[\Gamma]} ] ; A \wedge \dots \wedge A}} ; |[{\color{red} \overline{[\Gamma]}}] ; \Sigma_{1}^{{\color{red} \vec{w_{1}}}} \dots \Sigma_{n}^{{\color{red} \vec{w_{n}}}} } ; -[\distrule] ; (C \rightarrow \Sigma_{1}) \wedge \dots \wedge (C \rightarrow \Sigma_{n})}}
	& $\rightsquigarrow_{L}$ &
	\scalebox{0.9}{\drv{(C \rightarrow \Gamma) \wedge \Delta \wedge \drv[cyan]{\Omega ; |[\color{red} {[\Gamma]}] ; A \wedge \dots \wedge A} ; -[\switchrule] ; C^{\color{red} c} \rightarrow \drv[yellow]{\drv[green]{\Gamma^{\color{red} \vec{x}} \wedge \Delta \wedge A \dots A} ; |[\color{red} { \overline{[\Gamma]}}] ; \Sigma_{1}^{{\color{red} \vec{w_{1}}}} \dots \Sigma_{n}^{{\color{red} \vec{w_{n}}}} } ; -[\distrule] ; (C \rightarrow \Sigma_{1}) \wedge \dots \wedge (C \rightarrow \Sigma_{n})}}
\end{tabular}
\end{center}
The second part of the rewrite rule justifies the need for the book-keeping operation. In the rewrite below, let $A$ be the type of a variable $z$ where $z \in \vec{z}$. After lifting, we want to remove the variable from the cover as to ensure correctness since the variables in the cover denote the variables captured by the environment. Book-keeping allows us to remove these variables simultaneously.
\begin{center}
\begin{tabular}{c c c}
	\scalebox{0.8}{\drv{(C \rightarrow \Gamma^{\color{red} \vec{x}}) \wedge \Delta \wedge A ; -[\switchrule] ; C^{\color{red} c} \rightarrow \drv[yellow]{\drv[green]{\Gamma \wedge \Delta ; |[{\color{red} \overline{[\Gamma]}}] ; \Sigma_{1} \wedge \dots \wedge \Sigma_{n}} \wedge A^{{\color{red} z}} ; . ; \Sigma_{1} \wedge \dots \wedge \Sigma_{i} \wedge A \wedge \dots \wedge \Sigma_{n}}  ; -[\distrule] ; \dots \wedge (C^{{\color{red} e_{i}}} \rightarrow \Sigma_{i}^{{\color{red} \vec{w}}} \wedge A) \wedge \dots}}
	& $\rightsquigarrow$ &
	\scalebox{0.9}{\drv{\drv{(C \rightarrow \Gamma^{\color{red} \vec{x}}) \wedge \Delta ; -[\switchrule] ; C^{\color{red} c} \rightarrow \drv[yellow]{\drv[green]{\Gamma \wedge \Delta ; |[{\color{red} \overline{[\Gamma]}}] ; \Sigma_{1} \wedge \dots \wedge \Sigma_{n}} ; . ; \Sigma_{1} \wedge \dots \wedge \Sigma_{i} \wedge \dots \wedge \Sigma_{n}}  ; -[\distrule] ; \dots \wedge (C \rightarrow \Sigma_{i}) \wedge \dots}\wedge A^{\color{red} z} ; . ; \dots \wedge \drv[cyan]{(C^{\color{red} e_{i}} \rightarrow \Sigma_{i}^{{\color{red} \vec{w} }}) \wedge A ; -[\switchrule] ; C \rightarrow \Sigma_{i} \wedge A} \wedge \dots}}
\end{tabular}
\end{center}

The lifting rules ($l_{i}$) are justified by the need to lift closures out of the distributor, as opposed to duplicating them. The second set of rewrite rules, consecutive sharings are compounded and unary sharings are applied as substitutions. For simplicity, in the equivalent proof rewrite step we only show the binary case. 
\begin{align}
\share{\share{u}{\vec{w}}{y}}{y \cdot \vec{y}}{t} & \rightsquigarrow_{C} \share{u}{\vec{w} \cdot \vec{y}}{t}   \label{red:mergeshare} \tag{$c_{1}$} \\
\share{u}{x}{t} & \rightsquigarrow_{C} \sub{u}{t}{x} \label{red:compshare} \tag{$c_{2}$}
\end{align}
\begin{center}
\begin{tabular}{c c}
	{\small \drv{A ; -[\sharerule] ; A \wedge \drv[yellow]{A ; -[\sharerule] ; A \wedge A}}} \hspace{0.1cm} $\rightsquigarrow_{C}$ \hspace{0.1cm} {\small \drv{A ; -[\sharerule] ; A \wedge A \wedge A}} \hspace{0.5cm} & 
{\small \drv{A ; -[\sharerule] ; A}} \hspace{0.1cm} $\rightsquigarrow_{C}$ \hspace{0.1cm} {\small \drv{A}}
\end{tabular}
\end{center}
The atomic steps for duplicating are given in the third and final set of rewrite rules. The first being the atomic duplication step of an application, which is the same rule used in \cite{Gundersen-Heijltjes-Parigot-2013-LICS}. The binary case proof rewrite steps for each rule are also provided. There are also shown graphically in (respectively) Figure \ref{fig:duplcapp} (where we maintain links between sharings and unsharings), Figure \ref{fig:distintro}, and Figure \ref{fig:distelim} (where the unsharing node is linked to exactly one connecting sharing node).
\begin{equation} \label{red:appdup} \tag{$d_{1}$}
\share{u}{x_{1} \dots x_{n}}{\app{s}{t}} \rightsquigarrow_{D} \share{\share{\sub{\sub{u}{\app{z_{1}}{y_{1}}}{x_{1}}\dots}{\app{z_{n}}{y_{n}}}{x_{n}}}{z_{1} \dots z_{n}}{s}}{y_{1} \dots y_{n}}{t}
\end{equation}
\begin{center}
{\small \drv{(A \rightarrow B) \wedge A ; -[\apprule] ; B ; -[\sharerule] ; B \wedge B}}
\hspace{0.25cm}
$\rightsquigarrow_{D}$
\hspace{0.25cm}
{\small \drv{\drv{(A \rightarrow B) ; -[\sharerule] ; (A \rightarrow B) \wedge (A \rightarrow B)} \wedge \drv{B ; -[\sharerule] ; B \wedge B} ; . ; \drv{(A \rightarrow B) \wedge A ; -[\apprule] ; B} \wedge \drv{(A \rightarrow B) \wedge A ; -[\apprule] ; B}}}
\end{center}
\begin{equation} \label{red:absdup} \tag{$d_{2}$}
\begin{multlined}
\share{u}{x_{1}, \dots, x_{n}}{\fake{c}{\vec{y}}{t}} \rightsquigarrow_{D} \\
\sub{u}{\fake{e_{i}}{w_{i}}{w_{i}}}{x_{i}}_{i \in [n]} \dist{}{\fakedist{e_{1}}{w_{1}} \dots \fakedist{e_{n}}{w_{n}}}{\share{}{w_{1}, \dots, w_{n}}{t}}{c}{\vec{y}}
\end{multlined}
\end{equation}
\begin{center}
{\small \drv{(A \rightarrow B) \wedge \Gamma ; -[\switchrule] ; A \rightarrow \drv[yellow]{B \wedge \Gamma ; | ; C} ; -[\sharerule] ; (A \rightarrow C) \wedge (A \rightarrow C)}}
\hspace{0.25cm}
$\rightsquigarrow_{D}$
\hspace{0.25cm}
{\small \drv{(A \rightarrow B) \wedge \Gamma ; -[\switchrule] ; A \rightarrow \drv[yellow]{B \wedge \Gamma ; | ; C ; -[\sharerule] ; C \wedge C} ; -[\distrule] ; (A \rightarrow C) \wedge (A \rightarrow C)}}
\end{center}
\begin{equation} \label{red:distelim} \tag{$d_{3}$}
\dist{u}{\fakedist{e_{1}}{\vec{w_{1}}} \dots \fakedist{e_{n}}{\vec{w_{n}}}}{\share{}{\vec{w_{1}}, \dots, \vec{w_{n}}}{c}}{c}{c} \rightsquigarrow_{D} \exor{\exor{u}{e_{1}}{\vec{w_{1}}} \dots}{e_{n}}{\vec{w_{n}}}
\end{equation}
\begin{center}
{\small \drv{ ; -[a] ; A \rightarrow \drv{A ; -[\sharerule] ; A \wedge A} ; -[\distrule] ; (A \rightarrow A) \wedge (A \rightarrow A)}}
\hspace{0.25cm}
$\rightsquigarrow_{D}$
\hspace{0.25cm}
{\small \drv{\drv{ ; -[a] ; A \rightarrow A} \wedge \drv{ ; -[a] ; A \rightarrow A}}}
\end{center}

%\noindent As an example, observe $\share{u}{z_{1}, z_{2}}{\abs{x}{\app{(\abs{z}{z})}{\abs{y}{\app{(\app{y}{y})}{x}}}}}$ (note $\abs{x}{t} \equiv \fake{x}{x}{t}$). By (\ref{red:absdup}) we obtain $\dist{u'}{\fakedist{e_{1}}{z_{1}}, \fakedist{e_{2}}{z_{2}}}{\share{}{z_{1}, z_{2}}{\app{(\abs{z}{z})}{\abs{y}{\app{(\app{y}{y})}{x}}}}}{x}{x}$ where $u' = u\sub{}{\fake{e_{i}}{z_{i}}{z_{i}}}{z_{i}}_{i \in [2]}$. Then by reductions (\ref{red:appdup}, \ref{red:distshare}), we obtain the distributor $\dist{u''}{\fakedist{e_{1}}{z_{1}}, \fakedist{e_{2}}{z_{2}}}{\share{}{z_{1}, z_{2}}{\abs{y}{\app{(\app{y}{y})}{x}}}}{x}{x}$ where $u'' = u\sub{}{\fake{e_{i}}{z_{i}}{\app{a_{i}}{z_{i}}}}{z_{i}}_{i \in [2]}$. Then by (\ref{red:absdup}, \ref{red:appdup}, \ref{red:distshare}, \ref{red:distshare}) we obtain the distributor $\dist{u'''}{\fakedist{e_{1}}{z_{1}}, \fakedist{e_{2}}{z_{2}}}{\share{}{z_{1}, z_{2}}{x}}{x}{x}$ which can be eliminated by (\ref{red:distelim}). A full example can be found in the Appendix. 

\begin{example} %In the following, we apply the rules (\ref{red:absdup}, \ref{red:absdup}, \ref{red:appdup}, \ref{red:distshare}, \ref{red:distshare}, \ref{red:distelim}). We make the spine red to show that it is duplicated. The subterm not part of the spine is in green. The covers of the created phantom-variables are updated everytime we perform (\ref{red:distshare}). After the second time we perform this lifting, we remove ${\color{myGreen} d_{i}}$ from the cover of ${\color{red}x_{i}}$, indicating the term ${\color{myGreen} d_{i}}$ represents does not contain ${\color{red}x}$ as a bound variable and therefore should not contain ${\color{red}x_{i}}$. We are able to eliminate the $\fakedist{x}{x}$ distributor without duplicating the subterm ${\color{myGreen} \app{(\abs{z}{z})}{y} }$. 
The following example, illustrated in Figure \ref{fig:reduction}, is a reduction in the term calculus where we duplicate the {\red spine} of the term $\trm{[a_1,a_2-   {{\color{red} \lambda x .\lambda y. ( }} {{ \color{myGreen} (\lambda z.z)y}} {{ \color{red} ) x }} ]}$.
\begin{align*}
& \rightsquigarrow_{D} \trm{\{ {{ \color{red}x_1 }}<b_1>.b_1/a_1\}\{ {{ \color{red} x_2 }}<b_2>.b_2/a_2\}[x_1<b_1>,x_2<b_2>\mid x<x>[b_1,b_2- {{\color{red}\lambda y.(}}{{\color{myGreen}(\lambda z.z)y}}{{\color{red})x}}]]}
\\&\rightsquigarrow_{D} \trm{\{{{\color{red}x_1}}<c_1>.{{\color{red}y_1}}<c_1>c_1/a_1\}\{{{\color{red}x_2}}<c_2>.{{\color{red}y_2}}<c_2>.c_2/a_2\}}
\\&\quad\qquad\trm{[x_1<c_1>,x_2<c_2>\mid x<x>[y_1<c_1>,y_2<c_2> \mid y<y>[c_1,c_2-{{ \color{red}(}}{{\color{myGreen}(\lambda z.z)y}}{{\color{red})x}}]]]}
\\&\rightsquigarrow_{D} \trm{\{ {{\color{red}x_1}}<{{\color{myGreen} d_1}},e_1>. {{\color{red} y_1}}<{{\color{myGreen} d_1}},e_1>{{\color{myGreen} d_1}}e_1/a_1\}\{{{\color{red} x_2}}<{{\color{myGreen} d_2}},e_2>.{{\color{red} y_2}}<{{\color{myGreen} d_2}},e_2>.{{\color{myGreen} d_2}}e_2/a_2\}}
\\&\quad\qquad\trm{[x_1<{{\color{myGreen} d_1}},e_1>,x_2<{{\color{myGreen} d_2}},e_2>\mid x<x>[y_1<{{\color{myGreen} d_1}},e_1>,y_2<{{\color{myGreen} d_2}},e_2> \mid y<y>[{{\color{myGreen}d_1}},{{\color{myGreen}d_2}}-(\lambda z.z)y][e_1, e_2-{{\color{red} x}}]]]}
\\&\rightsquigarrow_{L} \trm{\{{{\color{red}x_1}}<{{\color{myGreen} d_1}},e_1>.{{\color{red}y_1}}<{{\color{myGreen} d_1}}>{{\color{myGreen} d_1}}e_1/a_1\}\{ {{\color{red}x_2}}<{{\color{myGreen} d_2}},e_2>. {{\color{red}y_2}}<{{\color{myGreen} d_2}}>.{{\color{myGreen} d_2}}e_2/a_2\}}
\\&\quad\qquad\trm{[x_1<{{\color{myGreen} d_1}},e_1>,x_2<{{\color{myGreen} d_2}},e_2>\mid x<x>[y_1<{{\color{myGreen} d_1}}>,y_2<{{\color{myGreen} d_1}}> \mid y<y>[{{\color{myGreen} d_1}},{{\color{myGreen} d_2}}-(\lambda z.z)y]]\,[e_1,e_2-{{\color{red} x}}]]}
\\&\rightsquigarrow_{L} \trm{\{ {{\color{red} x_1}}<e_1>. {{\color{red} y_1}}<{{\color{myGreen} d_1}}>{{\color{myGreen} d_1}}e_1/a_1\}\{{{\color{red}x_2}}<e_2>.{{\color{red}y_2}}<{{\color{myGreen} d_2}}>.{{\color{myGreen} d_2}}e_2/a_2\}}
\\&\quad\qquad\trm{[x_1<e_1>,x_2<e_2>\mid x<x>[e_1,e_2-{{\color{red}x}}]]~[y_1<{{\color{myGreen} d_1}}>,y_2<{{\color{myGreen} d_2}}> \mid y<y>[{{\color{myGreen} d_1}},{{\color{myGreen} d_2}}(\lambda z.z)y]]}
\\&\rightsquigarrow_{D} \trm{\{ {{\color{red} \lambda x_1}} . {{\color{red} y_1}}<{{\color{myGreen} d_1}}>{{\color{myGreen} d_1}}{{\color{red} x_1}}/a_1\}\{  {{\color{red} \lambda x_2}}. {{\color{red} y_2}}<{{\color{myGreen} d_2}}>.{{\color{myGreen} d_2}}{{\color{red} x_2}}/a_2\}~[y_1<{{\color{myGreen} d_1}}>,y_2<{{\color{myGreen} d_2}}> \mid y<y>[{{\color{myGreen} d_1}},{{\color{myGreen} d_2}}-(\lambda z.z)y]]}
\end{align*}


\end{example}
Reduction ($\rightsquigarrow_{(L, C, D, \beta)}$) preserves the conclusion of the derivation, and thus the following proposition is easy to observe.

\begin{proposition}
If $s \rightsquigarrow_{(L, C, D, \beta)} t$ and $s : A$, then $t : A$.
\end{proposition}

\begin{definition}
For a term $t \in \FALC$, if there does not exists a term $s \in \FALC$ such that $t \rightsquigarrow_{(L, C, D)} s$ then it is said that $t$ is in \defn{sharing normal form}.
\end{definition}

\noindent The following Lemma not only proves we have good translations in Section \ref{sect:compandreadback}, and shows duplication preserves denotation.

\begin{lemma}
\label{lem:preservesdenotation}
For a $t \in \FALC$ in sharing normal form and a $N \in \Lambda$.
\[ \readback{\compile{N}}{I} = N \hspace{1.5cm} \compile{\readback{t}{I}} = t \hspace{1.5cm} \exists_{ M \in \Lambda} . t = \compile{M} \] 
Otherwise if $s \rightsquigarrow_{(L, D, C)} t$ then $\readbackwmap{s}{\sigma}{\gamma} = \readbackwmap{t}{\sigma}{\gamma}$.
\end{lemma}

\begin{proof}
See~\cite{sherratt2019atomic}, proof of Lemma 24 and Lemma 25.
\end{proof}

\begin{lemma}
\label{lem:deliasnf}
Given a term $t \in \FALC$, then $\compile{\readbackclose{t}}$ is $t$ in sharing normal form. 
\end{lemma}
\begin{proof}
We can prove this by induction on the longest sharing reduction path from $t$. Our base case is already covered by Lemma \ref{lem:preservesdenotation}. We are then interested in the inductive case, where $t$ is not in sharing normal form. By Lemma \ref{lem:preservesdenotation}, $\readbackclose{t} = \readbackclose{t'}$ where $t \rightsquigarrow_{(D, L, C)} t'$. By induction hypothesis, $\compile{\readbackclose{t'}}$ is in sharing normal form. Hence $\compile{\readbackclose{t}}$ is in sharing normal form.  \QEDB
\end{proof}

\section{Strong Normalisation of Sharing Reductions}
\label{chap:snosr}

In order to show our calculus is strongly normalising, we first show that the sharing reduction rules are strongly normalising. We indite a measure on terms and show that this measure strictly decreases as sharing reduction progresses. Similar ideas and results can be found elsewhere: with \emph{memory} in \cite{Klop-1980}, the \emph{$\lambda$-$I$ calculus} in \cite{Barendregt-1984}, the \emph{$\lambda$-void calculus} \cite{Accattoli-Kesner-2012}, and the weakening $\lambda\mu$-calculus \cite{He-2018}. Our measure will consist of three components. First, the \defn{height} of a term is a multiset of integers, that measures the number of constructors from each sharing node to the root of the term in its graphical notation. The height is defined on terms as $\height{i}{-}$, where $i$ is an integer. We say $\height{}{t}$ for $\height{1}{t}$. We use $\cupdot$ to denote the disjoint union of two multisets. We denote $\height{i}{[\Gamma_{1}]} \cupdot \dots \cupdot \height{i}{[\Gamma_{n}]}$ as $\height{i}{\overline{[\Gamma]}}$ for the environment $\overline{[\Gamma]} = [\Gamma_{1}], \dots, [\Gamma_{n}]$.

\begin{definition}[Sharing Height]
The sharing height $\height{i}{t}$ of a term $t$ is given below, where $n$ is the number of closures in $\overline{[\Gamma]}$:
\begingroup
\allowdisplaybreaks
\begin{align*}
	\height{i}{x} &= \set{} &
	\height{i}{ \app{s}{t} } &= \height{i+1}{s} \cupdot \height{i+1}{t} \\
	\height{i}{\fake{c}{\vec{x}}{t}} &= \height{i + 1}{t} &
	\height{i}{t[\Gamma]} &= \height{i}{t} \cupdot \height{i}{[\Gamma]} \cupdot \set{i^{1}} \\
	\height{i}{\share{}{x_{1}, \dots, x_{n}}{t}} &= \height{i+1}{t} &
	\height{i}{\dist{}{\vecdist{e}{\vec{w}}}{\overline{[\Gamma]}}{c}{\vec{x}}} &= \height{i + 1}{\overline{[\Gamma]}} \cupdot \set{(i + 1)^{n}} 
\end{align*}
\endgroup
\end{definition}

\noindent This measure then strictly decreases for the rewrite rules \ref{red:liftappleft}, \ref{red:liftappright}, \ref{red:liftabs}, \ref{red:liftshare} and \ref{red:distshare}, i.e.\ if $t \rightsquigarrow_{L} u$ then $\height{i}{t} > \height{i}{u}$. The second measure we consider is the \defn{weight} of a term. Intuitively this quantifies the remaining duplications, which are performed with $\rightsquigarrow_{D}$ reductions. If a term would be deleted, we assign it with a weight `1' to express that it is not duplicated. Calculating the weight of a term requires an auxiliary function that assigns integer weights to the variables of a term. This auxiliary function is defined on terms $\weightvar{i}{-}$, where $i$ is an integer. To measure variables independently of binders is vital. It allows to measure distributors, which duplicate $\lambda$'s but not the bound variable. Also, only bound variables for abstractions are measured since variables bound by sharings are substituted in the interpretation.

\begin{definition}[Variable Weights]
The function $\weightvar{i}{t}$ returns a function that assigns integer weights to the free variables of $t$. It is defined by the below,  where $f = \weightvar{i}{t}$ and $g = f(x_{1}) + \dots + f(x_{n})$ for each $x_{i} \in \vec{x}$.
\begingroup
\allowdisplaybreaks
\begin{align*}
	\weightvar{i}{x} &= \set{x \mapsto i} &
	\weightvar{i}{\app{s}{t}} &= \weightvar{i}{s} \cupdot \weightvar{i}{t} \\
	\weightvar{i}{\fake{c}{c}{t}} &= \weightvar{i}{t} / \set{c} &
	\weightvar{i}{\fake{c}{\vec{x}}{t}} &= \weightvar{i}{t} \cupdot \set{c \mapsto i} \\
	\weightvar{i}{\share{t}{\vec{x}}{s}} &= \weightvar{i}{t} / \set{\vec{x}} \cupdot \weightvar{g}{s}  &
	\weightvar{i}{\share{t}{}{s}} &= \weightvar{i}{t} \cupdot \weightvar{1}{s} 
\end{align*}
\vspace{-0.8cm}
\begin{align*}
	\weightvar{i}{\dist{t}{\fakedist{e_{1}}{\vec{w_{1}}} \dots \fakedist{e_{n}}{\vec{w_{n}}}}{\overline{[\Gamma]}}{c}{c}} &= \weightvar{i}{t\overline{[\Gamma]}} / \set{c, e_{1}, \dots, e_{n}} \\
	\weightvar{i}{\dist{t}{\fakedist{e_{1}}{\vec{w_{1}}} \dots \fakedist{e_{n}}{\vec{w_{n}}}}{\overline{[\Gamma]}}{c}{\vec{x}}} &= \weightvar{i}{t\overline{[\Gamma]}} / \set{e_{1}, \dots, e_{n}} \cupdot \set{c \mapsto i}
\end{align*}
\endgroup
\end{definition}

\noindent The weight of a term can then be defined via the use of this auxiliary function. The auxiliary function is used when calculating the weight of a sharing, where the sharing weight of the variables bound by the sharing play a significant role in calculating the weight of the shared term. In the case of a weakening $[\leftarrow t]$, we assign an initial weight of $1$. Again we say $\weight{}{t} = \weight{1}{t}$.

\begin{definition}[Sharing Weight]
The sharing weight $\weight{i}{t}$ of a term $t$ is a multiset of integers computed by the function defined below,  where $f = \weightvar{i}{t}$ and $g = f(x_{1}) + \dots + f(x_{n})$ for each $x_{i} \in \vec{x}$.
\begingroup
\allowdisplaybreaks
\begin{align*}
	\weight{i}{x} &= \set{} &
	\weight{i}{\app{s}{t}} &= \weight{i}{s} \cupdot \weight{i}{t} \cupdot \set{i}\\
	\weight{i}{\fake{c}{c}{t}} &= \weight{i}{t} \cupdot \set{i} \cupdot \set{\weightvar{i}{t}(c)} &
	\weight{i}{\fake{c}{\vec{x}}{t}} &= \weight{i}{t} \cupdot \set{i} \\
	\weight{i}{\share{t}{\vec{x}}{s}} &= \weight{i}{t} \cupdot \weight{g}{s} &
	\weight{i}{\share{t}{}{s}} &= \weight{i}{t} \cupdot \weight{1}{s} 
\end{align*}
\vspace{-0.8cm}
\begin{align*}
	\weight{i}{\dist{t}{\fakedist{e_{1}}{\vec{w_{1}}} \dots \fakedist{e_{n}}{\vec{w_{n}}}}{\overline{[\Gamma]}}{c}{c}} &= \weight{i}{t\overline{[\Gamma]}} \cupdot \set{\weightvar{i}{t\overline{[\Gamma]}} (c)} \\ % \cupdot \set{f(e_{1}), \dots, f(e_{n})} \\
	\weight{i}{\dist{t}{\fakedist{e_{1}}{\vec{w_{1}}} \dots \fakedist{e_{n}}{\vec{w_{n}}}}{\overline{[\Gamma]}}{c}{\vec{x}}} &= \weight{i}{t\overline{[\Gamma]}}
\end{align*}
\endgroup
\end{definition}

\noindent This measure then strictly decreases on the rewrite rules \ref{red:appdup}, \ref{red:absdup}, \ref{red:distelim} and is unaffected by all the other sharing reduction rules, i.e.\ if $t \rightsquigarrow_{D} u$ then $\weight{i}{t} > \weight{i}{u}$. If $t \rightsquigarrow_{(L, C)} u$ then $\weight{i}{t} = \weight{i}{u}$. The third and last measure we consider is the \defn{number of closures} in the term,  where it can be easily observed that the rewrite rules \ref{red:mergeshare} and \ref{red:compshare} strictly decrease this measure, and that the $\rightsquigarrow_{L}$ rules do not alter the number of closures. We then use this along with height and weight to define a \emph{sharing measure} on terms.

\begin{definition}
\label{def:sharingmeasure}
The \defn{sharing measure} of a $\FALC$-term $t$ is a triple ($\weight{}{t}$, $\mathcal{C}$, $\height{}{t}$), where $\mathcal{C}$ is the number of closures in the term $t$. We compare sharing measures by using the lexicographical preferences according to $\mathcal{W}$ $>$ $\mathcal{C}$ $>$ $\mathcal{H}$.
\end{definition}

\begin{theorem}
\label{theo:sharingstronglynormal}
Sharing reduction $\rightsquigarrow_{(D, L, C)}$ is strongly normalising.
\end{theorem}

\noindent Now that we have proven the sharing reductions are strongly normalising, we can prove that they are confluent for closed terms.

\begin{theorem}
\label{theo:strongnormal}
The sharing reduction relation $\rightsquigarrow_{(D, L, C)}$ is confluent.
\end{theorem}
\begin{proof}
Lemma \ref{lem:preservesdenotation} tells us that the preservation is preserved under reduction i.e.\ for $s \rightsquigarrow_{(D, L, C)} t$, $\readback{s}{} = \readback{t}{}$. Therefore given $t \rightsquigarrow^{*}_{(D, L, C)} s_{1}$ and $t \rightsquigarrow^{*}_{(D, L, C)} s_{2}$, $\readback{t}{} = \readback{s_{1}}{} = \readback{s_{2}}{}$. Since we know that sharing reductions are strongly normalising, we know there exists terms $u_{1}$ and $u_{2}$ in sharing normal form such that $s_{1} \rightsquigarrow^{*}_{(D, L, C)} u_{1}$ and $s_{2} \rightsquigarrow^{*}_{(D, L, C)} u_{2}$. Lemma \ref{lem:preservesdenotation} tells us that terms in sharing normal form are in correspondence with their denotations i.e.\ $ \compile{\readback{t}{I}} = t $. Since by Lemma \ref{lem:preservesdenotation} we know $\readback{u_{1}}{} = \readback{s_{1}}{} = \readback{s_{2}}{} = \readback{u_{2}}{}$, and by Lemma \ref{lem:preservesdenotation} $ \compile{\readback{u_{1}}{I}} = u_{1} $ and $\compile{\readback{u_{2}}{I}} = u_{2}$, we can conclude $u_{1} = u_{2}$. Hence, we prove confluence. \QEDB
\end{proof}

\section{Preservation of Strong Normalisation and Confluence}
\label{chap:posn}

A $\beta$-step in our calculus may occur within a weakening, and therefore is simulated by zero $\beta$-steps in the $\lambda$-calculus. Therefore if there is an infinite reduction path located inside a weakening in $\FALC$, then the reduction path is not preserved in the corresponding $\lambda$-term as there are no weakenings. To deal with this, just as done in \cite{Accattoli-Kesner-2012,Gundersen-Heijltjes-Parigot-2013-LICS,He-2018}, we make use of the \defn{weakening calculus}. A $\beta$-step is non-deleting precisely because of the weakening construct. If a $\beta$-step would be deleting, then the weakening calculus would instead keep the deleted term around as `garbage', which can continue to reduce unless explicitly `garbage-collected' by extra (non-$\beta$) reduction steps. PSN has already be shown for the weakening calculus through the use of a perpetual strategy in \cite{Gundersen-Heijltjes-Parigot-2013-LICS}. A part of proving PSN is then using the weakening calculus to prove that if $t \in \FALC$ has a infinite reduction path, then its translation into the weakening calculus also has an infinite reduction path. 

\begin{definition}
\label{def:weakterms}
The $\weaksymbol$-terms of the weakening calculus ($\WEAK$) are
\begin{equation*}
	T, U, V \quad {:}{:}{=} \quad x \quad \vert \quad \abs{x}{T^{*}}\quad \vert \quad \app{U}{V} \quad \vert \quad \share{T}{}{U} \quad \vert \quad \bullet \hfill \text{(*) where $x \in \fv{T}$}
\end{equation*}
\end{definition}

The terms are variable, abstraction, application, weakening, and a bullet. In the weakening $\share{T}{}{U}$, the subterm $U$ is \emph{weakened}. The interpretation of atomic terms to weakening terms $\readweakwmap{-}{-}{-}$ can be seen as an extension of the translation into the $\lambda$-calculus (Definition \ref{def:readback}).

\begin{definition}
\label{def:transfalcweak}
	The interpretation $\readweakwmap{-}{-}{-} : \FALC \times (V \rightarrow \WEAK) \times (V \rightarrow V) \rightarrow \WEAK$ with maps $\sigma : V \rightarrow \WEAK$ and $\gamma : V \rightarrow V$ is defined as an extension of the translation in (Definition \ref{def:readback}) with the following additional special cases.
	\begingroup
	\allowdisplaybreaks
	\begin{align*}
		\readweakwmap{\share{u}{}{t}}{\sigma}{\gamma} &= \share{\readweakwmap{u}{\sigma}{\gamma}}{}{\readweakwmap{t}{\sigma}{\gamma}} \\
		\readweakwmap{\dist{u}{}{\overline{[\Gamma]}}{c}{c}}{\sigma}{\gamma} &= \readweakwmap{u \overline{[\Gamma]}}{\sigma[c \mapsto \bullet]}{\gamma} \\
		\readweakwmap{\dist{u}{}{\overline{[\Gamma]}}{c}{x_{1}, \dots, x_{n}}}{\sigma}{\gamma} &= \readweakwmap{u \overline{[\Gamma]}}{\sigma'}{\gamma}
	\end{align*}
where  $\sigma'(z) := \text{ if } z \in \set{x_{1}, \dots, x_{n}} \text{ then } \sigma(z) \sub{}{\bullet}{\gamma(c)}  \text{ else } \sigma(z)$
	\endgroup
\end{definition}

\noindent We say $\composeweak{t} = \readweakwmap{t}{I}{I}$ where $I$ is the identity function. We also have translations of the weakening calculus to and from the $\lambda$-calculus. Both of these translations were provided in \cite{Gundersen-Heijltjes-Parigot-2013-LICS}. The interpretation $\readbackweak{-}$ from weakening terms to $\lambda$-terms discards all weakenings. 

\begin{definition}
\label{def:transweak}
The interpretation $M \in \Lambda$, $\compweak{-} : \Lambda \rightarrow \WEAK$ is defined below.

\noindent
	$\compweak{x} = x$ \hspace{0.2cm}
	$\compweak{\app{M}{N}} = \app{\compweak{M}}{\compweak{N}}$ 
	\hspace{0.2cm}
	$\compweak{\abs{x}{N}} =
	\begin{cases}
		\abs{x}{\compweak{N}} & \text{if } x \in \fv{N} \\
		\abs{x}{\compweak{N} \share{}{}{x}} & \text{otherwise}
	\end{cases}$

\end{definition}
\noindent The following equalities can be observed, where $\sigma^{\Lambda}(z) = \readbackweak{\sigma^{\weaksymbol}(z)} $.
\begin{proposition}
\label{prop:equalterms}
For $N \in \Lambda$ and $t \in \FALC$ the following properties hold
\begin{center}
\begin{tabular}{c@{\hskip 0.5in} c@{\hskip 0.5in} c}
	$\readbackweak{\readweakwmap{t}{\sigma^{\weaksymbol}}{\gamma}} = \readbackwmap{t}{\sigma^{\Lambda}}{\gamma}$
	&
	$\composeweak{\compile{N}} = \compweak{N}$
	&
	$\readbackweak{\compweak{N}} = N$
\end{tabular}
\end{center}
 \noindent where for each $\set{x \mapsto M} \in \sigma^{\weaksymbol}$, $\set{x \mapsto \readbackweak{M}} \in \sigma^{\Lambda}$.
\end{proposition}

\begin{definition}
In the weakening calculus, $\beta$-reduction is defined as follows, where $\overline{[\Gamma]}$ are weakening constructs.
	$ \app{((\abs{x}{T}) \overline{[\Gamma]})}{U} \rightarrow_{\beta} \sub{T}{U}{x} \overline{[\Gamma]} $
\end{definition}

\begin{proposition}
\label{prop:willemresult}
 If $N \in \Lambda$ is strongly normalising, then so is $\compweak{N}$.
\end{proposition}

%\begin{proof}
%Provided in~\cite{Gundersen-Heijltjes-Parigot-2013-LICS}
%\end{proof}

When translating from $\FALC$ to $\WEAK$, weakenings are maintained whilst sharings are interpreted via substitution. Thus the reduction rules in the weakening calculus cover the spinal reductions for nullary distributors and weakenings.

\begin{definition}
Weakening reduction ($\rightarrow_{\weaksymbol}$) proceeds as follows.
%\begingroup
%\allowdisplaybreaks
%\begin{align*}
%	\tag{$\weaksymbol_{1}$}
%	\abs{x}{\share{T}{}{U}} &\rightarrow_{\weaksymbol} (\abs{x}{T})\share{}{}{U} \quad \text{if } x \not\in \fv{U} \\
%	\tag{$\weaksymbol_{2}$}
%	\app{\share{U}{}{T}}{V} &\rightarrow_{\weaksymbol} (\app{U}{V}) \share{}{}{T} \\
%	\tag{$\weaksymbol_{3}$}
%	\app{U}{\share{V}{}{T}} &\rightarrow_{\weaksymbol} (\app{U}{V}) \share{}{}{T} \\
%	\tag{$\weaksymbol_{4}$}
%	\share{T}{}{\share{U}{}{V}} &\rightarrow_{\weaksymbol} \share{T}{}{U} \share{}{}{V} \\
%	\tag{$\weaksymbol_{5}$}
%	\share{T}{}{\abs{x}{U}} &\rightarrow_{\weaksymbol} \share{T}{}{U \sub{}{\bullet}{x}} \\
%	\tag{$\weaksymbol_{6}$}
%	\share{T}{}{\app{U}{V}} &\rightarrow_{\weaksymbol} T \share{}{}{U} \share{}{}{V} \\
%	\tag{$\weaksymbol_{7}$}
%	\share{T}{}{\bullet} &\rightarrow_{\weaksymbol} T \\
%	\tag{$\weaksymbol_{8}$}
%	\share{T}{}{U} &\rightarrow_{\weaksymbol} T \quad \text{if $U$ is a subterm of $T$}
%\end{align*}
%\endgroup
\begingroup
\allowdisplaybreaks
\begin{align*}
	\app{\share{U}{}{T}}{V} &\rightarrow_{\weaksymbol} (\app{U}{V}) \share{}{}{T} &
	\app{U}{\share{V}{}{T}} &\rightarrow_{\weaksymbol} (\app{U}{V}) \share{}{}{T} \\
	\share{T}{}{\share{U}{}{V}} &\rightarrow_{\weaksymbol} \share{T}{}{U} \share{}{}{V} &
	\share{T}{}{\abs{x}{U}} &\rightarrow_{\weaksymbol} \share{T}{}{U \sub{}{\bullet}{x}} \\
	\share{T}{}{\app{U}{V}} &\rightarrow_{\weaksymbol} T \share{}{}{U} \share{}{}{V} &
	\share{T}{}{\bullet} &\rightarrow_{\weaksymbol} T \\
	\share{T}{}{U} &\rightarrow_{\weaksymbol} T^{(1)} &
	\abs{x}{\share{T}{}{U}} &\rightarrow_{\weaksymbol} (\abs{x}{T})\share{}{}{U}^{(2)} 
\end{align*}
{\small (1) if $U$ is a subterm of $T$} and {\small (2) if  $x \not\in \fv{U}$}
\endgroup
\end{definition}
\noindent It is easy to see that these rules correspond to special cases of the sharing reduction rules for $\FALC$. This resemblance is confirmed by the following Lemmas.

%It is not so obvious to see what the case ($\weaksymbol_{8}$) corresponds to. If $U$ is a subterm of $T$, then in the corresponding $\FALC$-term this term would be shared and one of the copies would be in a weakening. Thus this reduction relates to the case (\ref{red:mergeshare}), where we remove the weakening. We demonstrate by considering $t \share{}{}{y} \share{}{\vec{x} \cdot y \cdot \vec{z}}{u}  \rightsquigarrow_{C} t \share{}{\vec{x} \cdot \vec{z}}{u}$. On the left hand side, the corresponding weakening-term (obtained by $\compweak{-}$) would have the weakening $\share{}{}{U}$ where $U = \compweak{u}$. This is because $U$ is substituted into $\share{}{}{y}$, but on the right hand side this would be gone. This situation can only occur if there are other copies of $U$ substituted into the term. This corresponds to if only the corresponding (\ref{red:mergeshare}) reduction rule can occur.

\begin{lemma}
\label{lem:sharepreservebeta}
	If $t \rightsquigarrow_{\beta} u$ then $\composeweak{t} \rightarrow^{\plus}_{\beta} \composeweak{u}$.
\end{lemma}

\begin{lemma}
\label{theo:sharepreserve}
	If $t \rightsquigarrow_{(C, D, L)} u$ and for any $x \in \bv{t} \cup \fp{t}$ such that for all $z$, $x \not\in \fv{\sigma(z)}$.  $$\readweakwmap{t}{\sigma}{\gamma} \rightarrow^{*}_{\weaksymbol} \readweakwmap{u}{\sigma}{\gamma}$$

\end{lemma}

These Lemmas are proven in \cite[pp.~82-86]{sherratt2019atomic}. Now we show how $\FALC$ enjoys PSN with respect to the $\lambda$-calculus. 

%Recall that by Proposition \ref{prop:equalterms} that for all $N \in \Lambda$, $\composeweak{\compile{N}} = \compweak{N}$, and that Proposition \ref{prop:willemresult} states if a term $N \in \Lambda$ is strongly normalising then so is $\compweak{N}$. 

\begin{lemma}
\label{lem:infinitepath}
For $t \in \FALC$ has an infinite reduction path, then $\composeweak{t}$ also has an infinite reduction path.
\end{lemma}

\begin{proof}
Due to Theorem \ref{theo:strongnormal}, we know that the infinite reduction path contains infinite $\beta$-steps. This means in the reduction sequence, between each $\beta$-step, there are finite many $\rightsquigarrow_{(D, L, C)}$ reduction steps. Lemma \ref{theo:sharepreserve} says each $\rightsquigarrow_{(D, L, C)}$ step in $\FALC$ corresponds to zero or more weakening reductions ($\rightsquigarrow^{*}_{\weaksymbol}$). Lemma \ref{lem:sharepreservebeta} says that each beta step in $\FALC$ corresponds to one or more $\beta$-steps in $\WEAK$. Therefore, it must be that $\composeweak{t}$ also has an infinite reduction path. \QEDB
\end{proof}

\begin{theorem}
 If $N \in \Lambda$ is strongly normalising, then so is $\compile{N}$.
\end{theorem}

\begin{proof}
For a given $N \in \Lambda$ that is strongly normalising, we know by Lemma \ref{prop:willemresult} that $\compweak{N}$ is strongly normalising. Then $\composeweak{\compile{N}}$ is strongly normalising, since Proposition \ref{prop:equalterms} states that $\compweak{N} = \composeweak{\compile{N}}$. Then by Lemma \ref{lem:infinitepath}, which states that if $\composeweak{t}$ is strongly normalising, then $t$ is strongly normalising, proves that $\compile{N}$ is strongly normalising. \QEDB
\end{proof}

\noindent We now prove confluence, which is already known for the $\lambda$-calculus \cite{churchrossertheorem}. We first observe that a $\beta$-step in the $\lambda$-calculus is simulated in $\FALC$ by one $\beta$-step followed by zero or more sharing reductions. 

\begin{lemma}
\label{lem:simulatebetas}
Given $N, M \in \Lambda$. If $N \rightsquigarrow_{\beta} M$, then $\compile{N}  \rightsquigarrow_{\beta} \, \, \rightsquigarrow^{*}_{(D, L, C)} \compile{M}$.
\end{lemma}

\begin{proof}
This is proven by Sherratt in \cite[Lemma~67]{sherratt2019atomic}.
\end{proof}

\begin{theorem}
Given $t, s_{1}, s_{2} \in \FALC$. If $t \rightsquigarrow^{*}_{(\beta, D, L, C)} s_{1}$ and $t \rightsquigarrow^{*}_{(\beta, D, L, C)} s_{2}$, there exists a $u \in \FALC$ such that $s_{1} \rightsquigarrow^{*}_{(\beta, D, L, C)} u$ and $s_{2} \rightsquigarrow^{*}_{(\beta, D, L, C)} u$.

\end{theorem}

\begin{proof}
Suppose $t \rightsquigarrow^{*}_{(\beta, D, L, C)} s_{1}$ and $t \rightsquigarrow^{*}_{(\beta, D, L, C)} s_{2}$. Then we have $\readback{t}{} \rightsquigarrow^{*}_{\beta} \readback{s_{1}}{}$ and $\readback{t}{} \rightsquigarrow^{*}_{\beta} \readback{s_{2}}{}$. By the Church-Rosser theorem, there exists a $M \in \Lambda$ such that $\readback{s_{1}}{} \rightsquigarrow^{*}_{\beta} M$ and $\readback{s_{2}}{} \rightsquigarrow^{*}_{\beta} M$. Due to Lemma \ref{lem:deliasnf}, $\compile{\readback{s_{1}}{}} = s'_{1}$ and $\compile{\readback{s_{2}}{}} = s'_{2}$ where $s'_{1}, s'_{2} \in \FALC$ in sharing normal form. Then thanks to Lemma \ref{lem:simulatebetas} we know $s'_{1} \rightsquigarrow^{*}_{(\beta, D, L, C)} \compile{M}$ and $s'_{2} \rightsquigarrow^{*}_{(\beta, D, L, C)} \compile{M}$. Combined, we get confluence. \QEDB
\end{proof}

\section{Conclusion, related work, and future directions}
\label{chap:conc}

%We have studied the computational interpretation of the switch rule and discovered its correspondence with scope in the $\lambda$-calculus. We have studied the interaction between the switch and the medial rule, the two characteristic inference rules of deep inference. We interpret a calculus based on this interaction, which not only has the ability to duplicate terms atomically but can also duplicate solely the spine of an abstraction such that beta reduction can be applied on the duplicates. We show that this resulting calculus has natural properties with respect to the $\lambda$-calculus. 

%We have studied the computational interpretation of the switch rule and discovered its correspondence with scope in the $\lambda$-calculus. 

We have studied the interaction between the switch and the medial rule, the two characteristic inference rules of deep inference. We built a Curry--Howard interpretation based on this interaction, whose resulting calculus not only has the ability to duplicate terms atomically but can also duplicate solely the spine of an abstraction such that beta reduction can proceed on the duplicates. We show that this calculus has natural properties with respect to the $\lambda$-calculus.

This work, which started as an investigation into the Curry-Howard correspondence of the switch rule~\cite{sherratt2019atomic}, fits into a broader effort to give a computational interpretation to intuitionistic deep-inference proof theory. Br\"unnler and McKinley~\cite{Brunnler-McKinley-2008} give a natural reduction mechanism without medial (or switch), and observe that preservation of strong normalization fails. Guenot and Stra\ss burger~\cite{Guenot-Strassburger-2014} investigate a different switch rule, corresponding to the implication-left rule of sequent calculus. He~\cite{He-2018} extends the atomic $\lambda$-calculus to the lambda-mu-calculus. 

Our future goal is to develop the intuitionistic open deduction formalism towards optimal reduction~\cite{Levy-1980,Lamping-1990,Asperti-Guerrini-1998}, via the remaining medial and switch rules~\cite{Tiu-2006}.

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{SALC}
%

%%%%% To display Open Access text and logo, Please add below text and copy the cc_by_4-0.eps in the manuscript package %%%

\vfill

{\small\medskip\noindent{\bf Open Access} This chapter is licensed under the terms of the Creative Commons\break Attribution 4.0 International License (\url{http://creativecommons.org/licenses/by/4.0/}), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.}

{\small \spaceskip .28em plus .1em minus .1em The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material.~If material is not included in the chapter's Creative Commons license and your intended\break use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.}

\medskip\noindent\includegraphics{cc_by_4-0.eps}

\end{document}
